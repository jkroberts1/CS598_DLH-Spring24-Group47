{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jkroberts1/CS598_DLH-Spring24-Group47/blob/main/CS_598_DLH_Project_Evaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bRSC0cwq87Tb",
        "outputId": "9472facb-dd49-477c-a2f3-159fd6718fd9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/My Drive/CS598_DLH/Dataset2\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "%cd /content/drive/My Drive/CS598_DLH/Dataset2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WEq4lG7zT9bI"
      },
      "source": [
        "#Model Building\n",
        "\n",
        "This section will go over the implementation of the model\n",
        "\n",
        "Our Citation for the original paper is in the References section at [2].\n",
        "\n",
        "Original code is here: https://github.com/yejinjkim/synergy-transfer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "7h9NWwlT6JGL"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pdb\n",
        "import time\n",
        "import pickle\n",
        "import logging\n",
        "import matplotlib.pyplot as plt\n",
        "from utilities import Mapping\n",
        "\n",
        "\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "H8P0-gn_6Lgr"
      },
      "outputs": [],
      "source": [
        "data_path=''\n",
        "save_path=''\n",
        "\n",
        "bsz=128\n",
        "cuda=True\n",
        "device=3\n",
        "#torch.cuda.set_device(device)\n",
        "\n",
        "num_gene_compressed_drug=64\n",
        "num_gene_compressed_cell=128\n",
        "\n",
        "#isClassification=True #False for regression task\n",
        "syn_threshold=30\n",
        "ri_threshold=50\n",
        "\n",
        "log_interval=100\n",
        "epochs=10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YEytx-M66U3q"
      },
      "source": [
        "##Load data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "vTQEafe_6R7q"
      },
      "outputs": [],
      "source": [
        "#drug pair, cell, scoers\n",
        "df=pickle.load(open(data_path+'summary_mean.p', 'rb'))\n",
        "codes=pickle.load(open(data_path+'codes.p', 'rb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "XlygUsqC6bXQ",
        "outputId": "8d3d0521-15cd-4fab-f562-8ef9461c517c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   drug_row  drug_col  cell_line_name  study_id     ri_row     ri_col  \\\n",
              "0      2617      3003               0         4 -21.079400  17.392589   \n",
              "1      1515      3003               0         4  -4.051616  17.392589   \n",
              "2       904      1413               1        11 -71.949000   9.755000   \n",
              "3       904      3003               0         4  -9.231525  17.392589   \n",
              "4        63      1413               1        11 -12.272000  15.133000   \n",
              "\n",
              "   synergy_loewe  \n",
              "0       4.436431  \n",
              "1      10.755529  \n",
              "2      -6.188270  \n",
              "3       0.739056  \n",
              "4     -18.680123  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3864c0ff-1d18-4f5d-abb9-658c3ce4369b\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>drug_row</th>\n",
              "      <th>drug_col</th>\n",
              "      <th>cell_line_name</th>\n",
              "      <th>study_id</th>\n",
              "      <th>ri_row</th>\n",
              "      <th>ri_col</th>\n",
              "      <th>synergy_loewe</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2617</td>\n",
              "      <td>3003</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>-21.079400</td>\n",
              "      <td>17.392589</td>\n",
              "      <td>4.436431</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1515</td>\n",
              "      <td>3003</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>-4.051616</td>\n",
              "      <td>17.392589</td>\n",
              "      <td>10.755529</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>904</td>\n",
              "      <td>1413</td>\n",
              "      <td>1</td>\n",
              "      <td>11</td>\n",
              "      <td>-71.949000</td>\n",
              "      <td>9.755000</td>\n",
              "      <td>-6.188270</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>904</td>\n",
              "      <td>3003</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>-9.231525</td>\n",
              "      <td>17.392589</td>\n",
              "      <td>0.739056</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>63</td>\n",
              "      <td>1413</td>\n",
              "      <td>1</td>\n",
              "      <td>11</td>\n",
              "      <td>-12.272000</td>\n",
              "      <td>15.133000</td>\n",
              "      <td>-18.680123</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3864c0ff-1d18-4f5d-abb9-658c3ce4369b')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-3864c0ff-1d18-4f5d-abb9-658c3ce4369b button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-3864c0ff-1d18-4f5d-abb9-658c3ce4369b');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-b89ee386-1f41-44d0-81dd-a78bc539aafb\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-b89ee386-1f41-44d0-81dd-a78bc539aafb')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-b89ee386-1f41-44d0-81dd-a78bc539aafb button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "df.head()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "lIbIyQ03Rhjz"
      },
      "outputs": [],
      "source": [
        "# Drug's external features\n",
        "drug_features=pickle.load(open(data_path+'drug_features.p', 'rb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "d7U-QaJ_RiHj"
      },
      "outputs": [],
      "source": [
        "# Cell's external features\n",
        "cell_features=pickle.load(open(data_path+'cell_features.p', 'rb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "KYrPfecWRkmC"
      },
      "outputs": [],
      "source": [
        "num_celllines= len(codes['cell'].idx2item)\n",
        "num_drugs=len(codes['drugs'].idx2item)\n",
        "\n",
        "num_genes = len(codes['gene'].idx2item)\n",
        "num_tissue = len(codes['tissue'].idx2item)\n",
        "num_disease = len(codes['disease'].idx2item)\n",
        "\n",
        "num_drug_fp=len(drug_features.loc[0,'fps'])\n",
        "max_drug_sm_len = drug_features['smiles'].apply(lambda x: len(x)).max()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wDn92DrORnBE"
      },
      "source": [
        "#Gene compression\n",
        "This section will go over the dataloaders that cover the cell lines compression for the model training and the dataloaders that cover the drug target compression."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9qwIS_TRriS"
      },
      "source": [
        "##Dataset and Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "-lT9DSZiRtds"
      },
      "outputs": [],
      "source": [
        "class DrugTargetDataset(Dataset):\n",
        "    def __init__(self, drug_features):\n",
        "        self.drug_features = drug_features\n",
        "    def __len__(self):\n",
        "        return len(self.drug_features)\n",
        "    def __getitem__(self,idx):\n",
        "            gene_ids=self.drug_features.loc[idx, 'gene_id']\n",
        "            genes=np.zeros(num_genes)\n",
        "            genes[gene_ids]=1\n",
        "\n",
        "            return genes\n",
        "\n",
        "\n",
        "class CellGeneDataset(Dataset):\n",
        "    def __init__(self, cell_features):\n",
        "        self.cell_features = cell_features\n",
        "    def __len__(self):\n",
        "        return len(self.cell_features)\n",
        "    def __getitem__(self,idx):\n",
        "            gene_ids=self.cell_features.loc[idx,'gene_id']\n",
        "            genes = np.zeros(num_genes)\n",
        "            for key,value in gene_ids.items():\n",
        "                genes[key]=value\n",
        "\n",
        "            return genes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "pDH7O0TtR4Xa"
      },
      "outputs": [],
      "source": [
        "#Two layers of fully connected layers\n",
        "class FC2(nn.Module):\n",
        "    def __init__(self, in_features, out_features, dropout):\n",
        "        super(FC2, self).__init__()\n",
        "\n",
        "        self.bn = nn.BatchNorm1d(in_features)\n",
        "        self.fc1 = nn.Linear(in_features, int(in_features/2))\n",
        "        self.fc2 = nn.Linear(int(in_features/2),out_features)\n",
        "        self.dropout= nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.bn(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.dropout(F.relu(self.fc1(x)))\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kC5KtRZXR774"
      },
      "source": [
        "##Compress gene features\n",
        "Here is where the compression happens. For cell lines COSMIC and CCLE are combined.\n",
        "\n",
        "For drug targets, DrugBank, NIH-LINC, and TTD are combined."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "VOkZ_d_ZR6PV"
      },
      "outputs": [],
      "source": [
        "class GeneCompressor(nn.Module):\n",
        "    def __init__(self, num_in, num_out, dropout=0.1):\n",
        "        super(GeneCompressor, self).__init__()\n",
        "        self.dropout=dropout\n",
        "        self.encoder=nn.Linear(num_in, num_out)\n",
        "        self.decoder=nn.Linear(num_out,num_in)\n",
        "\n",
        "    def _encoder(self,x):\n",
        "        return F.dropout(F.relu(self.encoder(x)), self.dropout, training=self.training)\n",
        "\n",
        "    def _decoder(self,x):\n",
        "        return F.dropout(self.decoder(x), self.dropout, training=self.training)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x=self._encoder(x)\n",
        "        x=self._decoder(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "sj1jiclXSAo5"
      },
      "outputs": [],
      "source": [
        "def geneCompressing(data_loader,num_gene_compressed, noise_weight=0.2, epochs=20, log_interval=10 ):\n",
        "    #model\n",
        "    geneCompressor=GeneCompressor(num_genes, num_out=num_gene_compressed, dropout=0.1)\n",
        "    if cuda:\n",
        "        geneCompressor=geneCompressor.cuda()\n",
        "    criterion=nn.MSELoss()\n",
        "    optimizer=optim.Adam(geneCompressor.parameters())\n",
        "    for epoch in range(1,epochs+1):\n",
        "        #train\n",
        "        geneCompressor.train()\n",
        "        total_loss=0\n",
        "        start_time=time.time()\n",
        "        for iteration, gene in enumerate(data_loader):\n",
        "            gene=Variable(gene).float()\n",
        "            noise=noise_weight*torch.randn(gene.shape)\n",
        "\n",
        "            if cuda:\n",
        "                gene=gene.cuda()\n",
        "                noise=noise.cuda()\n",
        "            optimizer.zero_grad()\n",
        "            output=geneCompressor(gene+noise)\n",
        "            loss=criterion(output,gene)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.data\n",
        "            if iteration % log_interval == 0 and iteration > 0:\n",
        "                cur_loss = total_loss.item() / log_interval\n",
        "                elapsed = time.time() - start_time\n",
        "                print('| epoch {:3d} | {:5d}/{:5d} batches | ms/batch {:5.2f} | loss {:8.5f}'.format(epoch, iteration, int(len(data_loader)/bsz), elapsed * 1000/log_interval, cur_loss))\n",
        "                total_loss = 0\n",
        "                start_time = time.time()\n",
        "#         #test\n",
        "#         geneCompressor.eval()\n",
        "#         total_loss=0\n",
        "#         start_time=time.time()\n",
        "#         with torch.no_grad():\n",
        "#             for iteration, gene in enumerate(test_data_loader):\n",
        "#                 gene=Variable(gene).float()\n",
        "#                 if cuda:\n",
        "#                     gene=gene.cuda(device)\n",
        "#                 output=geneCompressor(gene)\n",
        "#                 loss=criterion(output,gene)\n",
        "#                 total_loss += loss.data\n",
        "#             print(total_loss.item()/iteration)\n",
        "    return geneCompressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pUd9aFAgSGTu",
        "outputId": "6da34275-467f-4103-86e9-aa7ed6d4acb5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch   1 |    10/    0 batches | ms/batch 99.24 | loss  0.00786\n",
            "| epoch   1 |    20/    0 batches | ms/batch 24.56 | loss  0.00518\n",
            "| epoch   1 |    30/    0 batches | ms/batch 26.08 | loss  0.00521\n",
            "| epoch   1 |    40/    0 batches | ms/batch 24.08 | loss  0.00482\n",
            "| epoch   2 |    10/    0 batches | ms/batch 24.55 | loss  0.00473\n",
            "| epoch   2 |    20/    0 batches | ms/batch 24.11 | loss  0.00414\n",
            "| epoch   2 |    30/    0 batches | ms/batch 23.87 | loss  0.00365\n",
            "| epoch   2 |    40/    0 batches | ms/batch 23.93 | loss  0.00325\n",
            "| epoch   3 |    10/    0 batches | ms/batch 26.85 | loss  0.00249\n",
            "| epoch   3 |    20/    0 batches | ms/batch 22.73 | loss  0.00200\n",
            "| epoch   3 |    30/    0 batches | ms/batch 24.10 | loss  0.00173\n",
            "| epoch   3 |    40/    0 batches | ms/batch 22.76 | loss  0.00149\n",
            "| epoch   4 |    10/    0 batches | ms/batch 26.11 | loss  0.00116\n",
            "| epoch   4 |    20/    0 batches | ms/batch 23.67 | loss  0.00091\n",
            "| epoch   4 |    30/    0 batches | ms/batch 22.52 | loss  0.00081\n",
            "| epoch   4 |    40/    0 batches | ms/batch 22.72 | loss  0.00072\n",
            "| epoch   5 |    10/    0 batches | ms/batch 26.20 | loss  0.00061\n",
            "| epoch   5 |    20/    0 batches | ms/batch 21.74 | loss  0.00049\n",
            "| epoch   5 |    30/    0 batches | ms/batch 23.51 | loss  0.00042\n",
            "| epoch   5 |    40/    0 batches | ms/batch 23.13 | loss  0.00042\n",
            "| epoch   6 |    10/    0 batches | ms/batch 43.16 | loss  0.00034\n",
            "| epoch   6 |    20/    0 batches | ms/batch 49.40 | loss  0.00034\n",
            "| epoch   6 |    30/    0 batches | ms/batch 70.86 | loss  0.00025\n",
            "| epoch   6 |    40/    0 batches | ms/batch 70.09 | loss  0.00026\n",
            "| epoch   7 |    10/    0 batches | ms/batch 73.79 | loss  0.00024\n",
            "| epoch   7 |    20/    0 batches | ms/batch 72.46 | loss  0.00020\n",
            "| epoch   7 |    30/    0 batches | ms/batch 50.39 | loss  0.00018\n",
            "| epoch   7 |    40/    0 batches | ms/batch 60.25 | loss  0.00025\n",
            "| epoch   8 |    10/    0 batches | ms/batch 53.68 | loss  0.00020\n",
            "| epoch   8 |    20/    0 batches | ms/batch 22.95 | loss  0.00016\n",
            "| epoch   8 |    30/    0 batches | ms/batch 24.32 | loss  0.00021\n",
            "| epoch   8 |    40/    0 batches | ms/batch 22.88 | loss  0.00017\n",
            "| epoch   9 |    10/    0 batches | ms/batch 27.57 | loss  0.00017\n",
            "| epoch   9 |    20/    0 batches | ms/batch 25.64 | loss  0.00015\n",
            "| epoch   9 |    30/    0 batches | ms/batch 23.15 | loss  0.00014\n",
            "| epoch   9 |    40/    0 batches | ms/batch 23.98 | loss  0.00015\n",
            "| epoch  10 |    10/    0 batches | ms/batch 26.26 | loss  0.00014\n",
            "| epoch  10 |    20/    0 batches | ms/batch 23.63 | loss  0.00015\n",
            "| epoch  10 |    30/    0 batches | ms/batch 22.28 | loss  0.00014\n",
            "| epoch  10 |    40/    0 batches | ms/batch 22.51 | loss  0.00013\n",
            "| epoch  11 |    10/    0 batches | ms/batch 25.22 | loss  0.00014\n",
            "| epoch  11 |    20/    0 batches | ms/batch 36.72 | loss  0.00014\n",
            "| epoch  11 |    30/    0 batches | ms/batch 43.79 | loss  0.00016\n",
            "| epoch  11 |    40/    0 batches | ms/batch 52.88 | loss  0.00012\n",
            "| epoch  12 |    10/    0 batches | ms/batch 45.77 | loss  0.00015\n",
            "| epoch  12 |    20/    0 batches | ms/batch 43.97 | loss  0.00012\n",
            "| epoch  12 |    30/    0 batches | ms/batch 23.14 | loss  0.00018\n",
            "| epoch  12 |    40/    0 batches | ms/batch 23.66 | loss  0.00015\n",
            "| epoch  13 |    10/    0 batches | ms/batch 25.66 | loss  0.00020\n",
            "| epoch  13 |    20/    0 batches | ms/batch 23.09 | loss  0.00013\n",
            "| epoch  13 |    30/    0 batches | ms/batch 22.94 | loss  0.00013\n",
            "| epoch  13 |    40/    0 batches | ms/batch 24.41 | loss  0.00013\n",
            "| epoch  14 |    10/    0 batches | ms/batch 37.14 | loss  0.00021\n",
            "| epoch  14 |    20/    0 batches | ms/batch 43.13 | loss  0.00012\n",
            "| epoch  14 |    30/    0 batches | ms/batch 57.98 | loss  0.00013\n",
            "| epoch  14 |    40/    0 batches | ms/batch 106.06 | loss  0.00012\n",
            "| epoch  15 |    10/    0 batches | ms/batch 56.58 | loss  0.00016\n",
            "| epoch  15 |    20/    0 batches | ms/batch 43.45 | loss  0.00011\n",
            "| epoch  15 |    30/    0 batches | ms/batch 39.23 | loss  0.00014\n",
            "| epoch  15 |    40/    0 batches | ms/batch 37.20 | loss  0.00012\n",
            "| epoch  16 |    10/    0 batches | ms/batch 27.39 | loss  0.00015\n",
            "| epoch  16 |    20/    0 batches | ms/batch 25.82 | loss  0.00012\n",
            "| epoch  16 |    30/    0 batches | ms/batch 23.11 | loss  0.00013\n",
            "| epoch  16 |    40/    0 batches | ms/batch 24.31 | loss  0.00014\n",
            "| epoch  17 |    10/    0 batches | ms/batch 26.26 | loss  0.00016\n",
            "| epoch  17 |    20/    0 batches | ms/batch 24.80 | loss  0.00011\n",
            "| epoch  17 |    30/    0 batches | ms/batch 22.07 | loss  0.00013\n",
            "| epoch  17 |    40/    0 batches | ms/batch 23.16 | loss  0.00013\n",
            "| epoch  18 |    10/    0 batches | ms/batch 24.49 | loss  0.00015\n",
            "| epoch  18 |    20/    0 batches | ms/batch 24.41 | loss  0.00013\n",
            "| epoch  18 |    30/    0 batches | ms/batch 22.82 | loss  0.00018\n",
            "| epoch  18 |    40/    0 batches | ms/batch 21.40 | loss  0.00012\n",
            "| epoch  19 |    10/    0 batches | ms/batch 27.31 | loss  0.00015\n",
            "| epoch  19 |    20/    0 batches | ms/batch 22.08 | loss  0.00014\n",
            "| epoch  19 |    30/    0 batches | ms/batch 23.24 | loss  0.00018\n",
            "| epoch  19 |    40/    0 batches | ms/batch 23.24 | loss  0.00013\n",
            "| epoch  20 |    10/    0 batches | ms/batch 25.41 | loss  0.00018\n",
            "| epoch  20 |    20/    0 batches | ms/batch 22.63 | loss  0.00012\n",
            "| epoch  20 |    30/    0 batches | ms/batch 22.74 | loss  0.00015\n",
            "| epoch  20 |    40/    0 batches | ms/batch 23.64 | loss  0.00013\n"
          ]
        }
      ],
      "source": [
        "\n",
        "#drug's target gene data\n",
        "drugGeneDataset=DrugTargetDataset(drug_features)\n",
        "drugGeneDataset_loader = DataLoader(drugGeneDataset, batch_size=64, shuffle=True)\n",
        "#learn\n",
        "drugGeneCompressor=geneCompressing(drugGeneDataset_loader, num_gene_compressed_drug)\n",
        "#save\n",
        "drugGeneCompressor.eval()\n",
        "drugGeneCompressed=np.array([drugGeneCompressor.cpu()._encoder(torch.FloatTensor(drugGeneDataset[d])).data.numpy() for d in range(num_drugs)])\n",
        "torch.save(drugGeneCompressor.state_dict(), data_path+'drugGeneCompressor.p')\n",
        "pickle.dump(drugGeneCompressed, open(data_path+'drugGeneCompressed.p', 'wb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "ltP2Vba6SIqD"
      },
      "outputs": [],
      "source": [
        "\n",
        "drugGeneCompressed=pickle.load(open(data_path+'drugGeneCompressed.p', 'rb'))\n",
        "drugGeneCompressed=torch.FloatTensor(drugGeneCompressed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uK_mEkS8SK9v",
        "outputId": "b0a9e78f-5911-4126-da2c-e2dc00875f46"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch   1 |     1/    0 batches | ms/batch 414.69 | loss  2.74390\n",
            "| epoch   2 |     1/    0 batches | ms/batch 422.43 | loss  3.53132\n",
            "| epoch   3 |     1/    0 batches | ms/batch 416.57 | loss  2.99976\n",
            "| epoch   4 |     1/    0 batches | ms/batch 406.06 | loss  2.83008\n",
            "| epoch   5 |     1/    0 batches | ms/batch 201.79 | loss  2.40781\n",
            "| epoch   6 |     1/    0 batches | ms/batch 213.83 | loss  2.27313\n",
            "| epoch   7 |     1/    0 batches | ms/batch 237.67 | loss  2.26563\n",
            "| epoch   8 |     1/    0 batches | ms/batch 219.95 | loss  2.07515\n",
            "| epoch   9 |     1/    0 batches | ms/batch 216.21 | loss  2.00170\n",
            "| epoch  10 |     1/    0 batches | ms/batch 219.87 | loss  1.95035\n",
            "| epoch  11 |     1/    0 batches | ms/batch 218.58 | loss  1.74026\n",
            "| epoch  12 |     1/    0 batches | ms/batch 218.08 | loss  1.77221\n",
            "| epoch  13 |     1/    0 batches | ms/batch 208.76 | loss  1.78408\n",
            "| epoch  14 |     1/    0 batches | ms/batch 208.43 | loss  1.71462\n",
            "| epoch  15 |     1/    0 batches | ms/batch 203.65 | loss  1.66737\n",
            "| epoch  16 |     1/    0 batches | ms/batch 204.53 | loss  1.66733\n",
            "| epoch  17 |     1/    0 batches | ms/batch 244.21 | loss  1.67430\n",
            "| epoch  18 |     1/    0 batches | ms/batch 436.34 | loss  1.59274\n",
            "| epoch  19 |     1/    0 batches | ms/batch 260.93 | loss  1.54862\n",
            "| epoch  20 |     1/    0 batches | ms/batch 457.43 | loss  1.65043\n"
          ]
        }
      ],
      "source": [
        "#cell line's gene expression data\n",
        "cellGeneDataset=CellGeneDataset(cell_features)\n",
        "cellGeneDataset_loader = DataLoader(cellGeneDataset, batch_size=64, shuffle=True)\n",
        "#learn\n",
        "cellGeneCompressor=geneCompressing(cellGeneDataset_loader,num_gene_compressed_cell, noise_weight=0.01, log_interval=1 )\n",
        "#save\n",
        "cellGeneCompressor.eval()\n",
        "cellGeneCompressed=np.array([cellGeneCompressor.cpu()._encoder(torch.FloatTensor(cellGeneDataset[d])).data.numpy() for d in range(num_celllines)])\n",
        "torch.save(cellGeneCompressor.state_dict(), data_path+'cellGeneCompressor.p')\n",
        "pickle.dump(cellGeneCompressed, open(data_path+'cellGeneCompressed.p', 'wb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "5baTuGAiSNWy"
      },
      "outputs": [],
      "source": [
        "cellGeneCompressed=pickle.load(open(data_path+'cellGeneCompressed.p', 'rb'))\n",
        "cellGeneCompressed=torch.FloatTensor(cellGeneCompressed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1G1SjzOSPZ3"
      },
      "source": [
        "#Synergy prediction\n",
        "\n",
        "Here is where the train test splits are created for the model training and evalutation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uBzRQOi0ST1z"
      },
      "source": [
        "##Train/test split in cross or external validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "oWvmAB4OSXs8"
      },
      "outputs": [],
      "source": [
        "def get_cell_of_interest(tissues):\n",
        "    tissues_of_interests = [codes['tissue'].item2idx[minor_tissue] for minor_tissue in tissues]\n",
        "    cell_of_interest = cell_features.index[cell_features['tissue_id'].isin(tissues_of_interests)].tolist()\n",
        "    return cell_of_interest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_hNX2nZuSb6y"
      },
      "source": [
        "Train/test for general model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "ehqDwjQeSaEi"
      },
      "outputs": [],
      "source": [
        "minor_tissues=['bone', 'prostate' ]\n",
        "cell_of_interest = get_cell_of_interest(minor_tissues)\n",
        "df_tissue_of_interest = df.loc[df['cell_line_name'].isin(cell_of_interest),:]\n",
        "df_all = df.drop(df_tissue_of_interest.index)\n",
        "#specific database\n",
        "#df_all = df_all[df_all['study_id'].isin([1,3])]\n",
        "df_all = df_all.dropna()\n",
        "#cross validation\n",
        "df_train, df_test = train_test_split(df_all, test_size=0.2) #cross validation\n",
        "#external validation\n",
        "#df_train=df_all.loc[df_all['study_id']==3] 3: 'ALMANAC'\n",
        "#df_test=df_all.loc[df_all['study_id']==1] 1: 'ONEIL'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0cjfz3rASfTs"
      },
      "source": [
        "Train/test for bone\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "mpvPCY9vSiKA"
      },
      "outputs": [],
      "source": [
        "_df_bone = df.loc[df['cell_line_name'].isin(get_cell_of_interest(['bone'])),:]\n",
        "# Cross validation\n",
        "_df_train_bone, _df_test_bone = train_test_split(_df_bone, test_size=0.2, random_state=1)\n",
        "# External validation\n",
        "_df_train_bone=_df_bone.loc[_df_bone['study_id'].isin([7,8,9])]\n",
        "_df_test_bone= _df_bone.loc[_df_bone['study_id'].isin([7,8,9])]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lvw7s8qFSj4q"
      },
      "source": [
        "Train/test for prostate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Zajh02egSmv_"
      },
      "outputs": [],
      "source": [
        "_df_prostate= df.loc[df['cell_line_name'].isin(get_cell_of_interest(['prostate'])),:]\n",
        "#Cross validation\n",
        "_df_train_prostate, _df_test_prostate = train_test_split(_df_prostate, test_size=0.2, random_state=1)\n",
        "# External validation\n",
        "_df_train_prostate=_df_prostate.loc[_df_prostate['study_id'].isin([1,3])]\n",
        "_df_test_prostate=_df_prostate.loc[_df_prostate['study_id'].isin([1,3])]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yyK8Oiq2SosA"
      },
      "source": [
        "##Dataset and Dataloader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "JsGHqpjRSt2v"
      },
      "outputs": [],
      "source": [
        "class DrugCombDataset(Dataset):\n",
        "    def __init__(self, df, drug_features, cell_features):\n",
        "        self.df = df\n",
        "        self.drug_features = drug_features\n",
        "        self.cell_features = cell_features\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        d1 = self.df.iloc[idx, 0]\n",
        "        d2 = self.df.iloc[idx, 1]\n",
        "        cell = self.df.iloc[idx,2]\n",
        "        ri_d1 = 1.0 if float(self.df.iloc[idx,3]) >ri_threshold else 0\n",
        "        ri_d2 = 1.0 if float(self.df.iloc[idx,4]) >ri_threshold else 0\n",
        "        syn = 1.0 if self.df.iloc[idx, 5] >syn_threshold else 0\n",
        "\n",
        "\n",
        "        #external features\n",
        "        d1_fp = np.array(self.drug_features.loc[d1, 'fps'])\n",
        "        d1_sm = self.drug_features.loc[d1, 'smiles']\n",
        "        d1_sm = np.pad(d1_sm, pad_width=(0, max_drug_sm_len-len(d1_sm)), mode='constant', constant_values=0)\n",
        "        d1_gn=drugGeneCompressed[d1]\n",
        "\n",
        "        d2_fp = np.array(self.drug_features.loc[d2, 'fps'])\n",
        "        d2_sm = self.drug_features.loc[d2, 'smiles']\n",
        "        d2_sm = np.pad(d2_sm, pad_width=(0, max_drug_sm_len-len(d2_sm)), mode='constant', constant_values=0)\n",
        "        d2_gn=drugGeneCompressed[d2]\n",
        "\n",
        "\n",
        "        c_ts = self.cell_features.loc[cell, 'tissue_id']\n",
        "        c_ds = self.cell_features.loc[cell, 'disease_id']\n",
        "\n",
        "        c_gn= cellGeneCompressed[cell]\n",
        "\n",
        "        sample = {\n",
        "            'd1': d1,\n",
        "            'd1_fp': d1_fp,\n",
        "            'd1_sm': d1_sm,\n",
        "            'd1_gn': d1_gn,\n",
        "\n",
        "            'd2': d2,\n",
        "            'd2_fp': d2_fp,\n",
        "            'd2_sm': d2_sm,\n",
        "            'd2_gn': d2_gn,\n",
        "\n",
        "            'cell': cell,\n",
        "            'c_ts': c_ts,\n",
        "            'c_ds': c_ds, #missing -1\n",
        "            'c_gn': c_gn,\n",
        "\n",
        "            'ri_d1': ri_d1,\n",
        "            'ri_d2': ri_d2,\n",
        "            'syn': syn\n",
        "        }\n",
        "\n",
        "        return sample\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9PkAeRM9S5_3"
      },
      "source": [
        "##Load dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9GrxImKS_1U"
      },
      "source": [
        "General model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "HgggQ5xxS950"
      },
      "outputs": [],
      "source": [
        "train = DrugCombDataset(df_train, drug_features, cell_features)\n",
        "train_loader = DataLoader(train, batch_size=bsz, shuffle=True)\n",
        "test = DrugCombDataset(df_test, drug_features, cell_features)\n",
        "test_loader = DataLoader(test, batch_size=bsz, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1E_raSWTDKw"
      },
      "source": [
        "Bone"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "VJVrESwoTFBR"
      },
      "outputs": [],
      "source": [
        "_train_bone = DrugCombDataset(_df_train_bone, drug_features, cell_features)\n",
        "_train_loader_bone = DataLoader(_train_bone, batch_size=bsz, shuffle=True)\n",
        "_test_bone = DrugCombDataset(_df_test_bone, drug_features, cell_features)\n",
        "_test_loader_bone = DataLoader(_test_bone, batch_size=bsz, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMrqfPVcTHIF"
      },
      "source": [
        "Prostate\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "Z7BfEM6hTI75"
      },
      "outputs": [],
      "source": [
        "_train_prostate = DrugCombDataset(_df_train_prostate, drug_features, cell_features)\n",
        "_train_loader_prostate = DataLoader(_train_prostate, batch_size=bsz, shuffle=True)\n",
        "_test_prostate = DrugCombDataset(_df_test_prostate, drug_features, cell_features)\n",
        "_test_loader_prostate = DataLoader(_test_prostate, batch_size=bsz, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FUu6PF6iTKsA"
      },
      "source": [
        "#Prediction model\n",
        "\n",
        "Our main training model will be called Comb(). For Comb() to work we need to include 2 other encoder models. These are DrugEncoder() and CellEncoder().\n",
        "\n",
        "DrugEncoder() loads from the the drug combined dataloader. The comments in the model go over how the model goes through the different parts of the input data in order to prep for the forward pass.\n",
        "\n",
        "CellEncoder() is much the same but uses the cell lines combined dataloader.\n",
        "\n",
        "Comb() is the main prediction model. It is made from the drug encoder and the cell encoder models. The forward pass encodes the initial drug, then the starting drug.\n",
        "\n",
        "The synergy is calculated by concatentation the two drug encoders and also the cell line encoder and passing them through a fully connected layer. The sensitivities for each drug encoding is also calculated by passing them through a fully connected layer after concatentation.\n",
        "\n",
        "Original Paper Repo: https://github.com/yejinjkim/synergy-transfer/tree/master"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "b8do4cF3TPdb"
      },
      "outputs": [],
      "source": [
        "class DrugEncoder(nn.Module):\n",
        "    def __init__(self,\n",
        "                 num_drugs=num_drugs,\n",
        "                 num_ID_emb=0,\n",
        "                 num_drug_fp=num_drug_fp,\n",
        "                 max_drug_sm_len=max_drug_sm_len,\n",
        "                 num_gene = num_gene_compressed_drug,\n",
        "                 num_comp_char=len(codes['mole'].idx2item),\n",
        "                 fp_embed_sz = 32,\n",
        "                 gene_embed_sz = int(num_gene_compressed_drug/2),\n",
        "                 out_size=64,\n",
        "                 dropout=0.3):\n",
        "        super(DrugEncoder, self).__init__()\n",
        "\n",
        "        self.dropout= dropout\n",
        "        #DRUG\n",
        "        #drug ID\n",
        "        #self.embed_id = nn.Embedding(num_drugs, num_ID_emb)\n",
        "\n",
        "        #compound ID\n",
        "        self.embed_comp = nn.Embedding(num_comp_char, num_comp_char, padding_idx=0)#padding's idx=0\n",
        "        #encoding compound\n",
        "        self.encoderlayer = nn.TransformerEncoderLayer(d_model=num_comp_char, nhead=4)\n",
        "        self.encoder = nn.TransformerEncoder(self.encoderlayer, num_layers=1)\n",
        "\n",
        "        #fingerprint\n",
        "        self.dense_fp = nn.Linear(num_drug_fp,fp_embed_sz)\n",
        "        #gene\n",
        "        self.dense_gene = nn.Linear(num_gene,gene_embed_sz)\n",
        "\n",
        "        #depthwise for compound encoding\n",
        "        self.conv = nn.Conv2d(1, 1, (1, num_comp_char), groups=1)\n",
        "\n",
        "        #combined\n",
        "        combined_sz = num_ID_emb+fp_embed_sz+max_drug_sm_len+gene_embed_sz\n",
        "        self.FC2 = FC2(combined_sz, out_size, dropout)\n",
        "\n",
        "    def forward(self, d_list):\n",
        "        \"\"\"\n",
        "            id: bsz*1\n",
        "            fp: bsz*num_drug_fp\n",
        "            sm: bsz*max_drug_sm_len\n",
        "        \"\"\"\n",
        "        id, fp, sm, gn = d_list\n",
        "\n",
        "        sm = self.embed_comp(sm) #bsz*max_drug_sm_len*num_comp_char(embedding size)\n",
        "        sm = self.encoder(sm)\n",
        "        sm = self.conv(sm.unsqueeze(1)).squeeze()\n",
        "\n",
        "        fp = F.relu(self.dense_fp(fp))\n",
        "        gn = F.relu(self.dense_gene(gn))\n",
        "\n",
        "        #combine\n",
        "        x = torch.cat((fp, sm, gn),1) # bsz*[num_emb_id+num_drug_fp+max+drug_sm]\n",
        "        x = self.FC2(x)\n",
        "\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "KCxo3wweTPtH"
      },
      "outputs": [],
      "source": [
        "class CellEncoder(nn.Module):\n",
        "    def __init__(self,\n",
        "                 num_cells=num_celllines,\n",
        "                 num_tissue=0,\n",
        "                 num_disease=num_disease,\n",
        "                 num_ID_emb=0,\n",
        "                 gene_embed_sz=int(num_gene_compressed_cell/2),\n",
        "                 num_gene=num_gene_compressed_cell,\n",
        "                 out_size=64,\n",
        "                 dropout=0.3):\n",
        "        super(CellEncoder, self).__init__()\n",
        "\n",
        "        self.dropout= dropout\n",
        "        #cell ID\n",
        "        #self.embed_id = nn.Embedding(num_cells, num_ID_emb)\n",
        "        #cell tissue\n",
        "        #self.embed_ts = nn.Embedding(num_tissue, num_tissue)\n",
        "        #cell disease\n",
        "        self.embed_ds = nn.Embedding(num_disease, num_disease, padding_idx=3)\n",
        "        #gene\n",
        "        self.dense_gene = nn.Linear(num_gene,gene_embed_sz)\n",
        "\n",
        "        #combined\n",
        "        combined_sz = num_ID_emb+num_tissue+num_disease+gene_embed_sz\n",
        "        self.FC2 = FC2(combined_sz, out_size, dropout)\n",
        "\n",
        "\n",
        "    def forward(self, c_list):\n",
        "        \"\"\"\n",
        "            id: bsz*1\n",
        "            fp: bsz*num_drug_fp\n",
        "            sm: bsz*max_drug_sm_len\n",
        "        \"\"\"\n",
        "        id, ts, ds, gn = c_list\n",
        "        ds = F.relu(self.embed_ds(ds)) #bsz*num_diesaes\n",
        "\n",
        "        gn = F.relu(self.dense_gene(gn)) #bsz*gene_embed_sz\n",
        "\n",
        "        #combine\n",
        "        x = torch.cat((ds, gn),1) # bsz*combined_sz\n",
        "        x = self.FC2(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "XYP76gyLTguS"
      },
      "outputs": [],
      "source": [
        "class Comb(nn.Module):\n",
        "    def __init__(self, num_cells=num_celllines,\n",
        "                 num_drugs=num_drugs,\n",
        "                 num_drug_fp=num_drug_fp,\n",
        "                 max_drug_sm_len=max_drug_sm_len,\n",
        "                num_comp_char=len(codes['mole'].idx2item),\n",
        "                 num_ID_emb=0,\n",
        "                 out_size=64,\n",
        "                dropout=0.3):\n",
        "\n",
        "        super(Comb, self).__init__()\n",
        "\n",
        "        self.dropout=dropout\n",
        "        #drug\n",
        "        self.drugEncoder = DrugEncoder()\n",
        "        #cell\n",
        "        self.cellEncoder = CellEncoder()\n",
        "        #fc\n",
        "        self.fc_syn = FC2(out_size*3, 1, dropout)\n",
        "        self.fc_ri = FC2(out_size*2, 1, dropout)\n",
        "\n",
        "    def forward(self, d1_list, d2_list, c_list):\n",
        "        d1 = self.drugEncoder(d1_list)\n",
        "        d2 = self.drugEncoder(d2_list)\n",
        "        c = self.cellEncoder(c_list)\n",
        "\n",
        "        syn = self.fc_syn(torch.cat((d1, d2, c),1))\n",
        "        ri1 = self.fc_ri(torch.cat((d1,c),1))\n",
        "        ri2 = self.fc_ri(torch.cat((d2,c),1))\n",
        "\n",
        "        return syn, ri1, ri2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCVSwwq3UOSn"
      },
      "source": [
        "#Training\n",
        "\n",
        "The model training is as follows. We take our Comb() model and begin training with a Binary Cross Entropy Loss. We use the Adagrad optimizer.\n",
        "\n",
        "For training we get all the training sample values from the training data loader and then we pass them into the model. The drug synergy is outputted by the model and we compute loss, compute the gradient, and iterate the model.\n",
        "\n",
        "##Hyperparams\n",
        "We have a few hyperparameters that we've taken from the paper.\n",
        "- Our learning rate is the default torch learning rate which is 0.001\n",
        "- Our batch size is 100\n",
        "- Our dropout is 0.1\n",
        "\n",
        "##Computational Requirements\n",
        "To run this script there are computational requirements.\n",
        "- We had CUDA set up using 3 devices on Google Colab Pro.\n",
        "- We trained for 10 epochs\n",
        "- The average runtime for each epoch was 66 ms per batch and we had 17 batches\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iOQ5i98CTk1M",
        "outputId": "2cf05421-093d-4259-cf6e-011b67148d18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
          ]
        }
      ],
      "source": [
        "model = Comb()\n",
        "if cuda:\n",
        "    model = model.cuda()\n",
        "#Regression\n",
        "#criterion_mse = nn.MSELoss()\n",
        "#Classification\n",
        "criterion_bce = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adagrad(model.parameters())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "1iryEwxGTmzH"
      },
      "outputs": [],
      "source": [
        "#Training\n",
        "def training(isAux, data_loader):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    start_time = time.time()\n",
        "\n",
        "    for iteration, sample in enumerate(data_loader):\n",
        "      try:\n",
        "        d1=Variable(sample['d1'])\n",
        "        d1_fp = Variable(sample['d1_fp'].float())\n",
        "        d1_sm = Variable(sample['d1_sm'])\n",
        "        d1_gn = Variable(sample['d1_gn'].float())\n",
        "\n",
        "        d2=Variable(sample['d2'])\n",
        "        d2_fp = Variable(sample['d2_fp'].float())\n",
        "        d2_sm = Variable(sample['d2_sm'])\n",
        "        d2_gn = Variable(sample['d2_gn'].float())\n",
        "\n",
        "        cell = Variable(sample['cell'])\n",
        "        c_ts = Variable(sample['c_ts'])\n",
        "        c_ds = Variable(sample['c_ds'])\n",
        "        c_gn = Variable(sample['c_gn'].float())\n",
        "\n",
        "        syn_true = Variable(sample['syn'].float())\n",
        "        ri_d1=Variable(sample['ri_d1'].float())\n",
        "        ri_d2=Variable(sample['ri_d2'].float())\n",
        "\n",
        "\n",
        "        if cuda:\n",
        "            d1=d1.cuda()\n",
        "            d1_fp=d1_fp.cuda()\n",
        "            d1_sm=d1_sm.cuda()\n",
        "            d1_gn=d1_gn.cuda()\n",
        "\n",
        "            d2=d2.cuda()\n",
        "            d2_fp=d2_fp.cuda()\n",
        "            d2_sm=d2_sm.cuda()\n",
        "            d2_gn=d2_gn.cuda()\n",
        "\n",
        "            cell=cell.cuda()\n",
        "            c_ts=c_ts.cuda()\n",
        "            c_ds=c_ds.cuda()\n",
        "            c_gn=c_gn.cuda()\n",
        "\n",
        "            syn_true=syn_true.cuda()\n",
        "            ri_d1=ri_d1.cuda()\n",
        "            ri_d2=ri_d2.cuda()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        syn, ri1, ri2 = model((d1, d1_fp, d1_sm, d1_gn), (d2, d2_fp, d2_sm, d2_gn), (cell, c_ts, c_ds, c_gn) )\n",
        "\n",
        "        if not isAux:\n",
        "            loss = criterion_bce(syn, syn_true.view(-1,1))\n",
        "        else:\n",
        "            loss = criterion_bce(ri1, ri_d1.view(-1,1))+criterion_bce(ri2, ri_d2.view(-1,1))\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.data\n",
        "\n",
        "        if iteration % log_interval == 0 and iteration > 0:\n",
        "            cur_loss = total_loss.item() / log_interval\n",
        "            elapsed = time.time() - start_time\n",
        "            print('| epoch {:3d} | {:5d}/{:5d} batches | ms/batch {:5.2f} | loss {:8.5f}'.format(epoch, iteration, int(len(train_loader)/bsz), elapsed * 1000/log_interval, cur_loss))\n",
        "\n",
        "            total_loss = 0\n",
        "            start_time = time.time()\n",
        "      except:\n",
        "          continue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "5fhPOgRTTqt3"
      },
      "outputs": [],
      "source": [
        "\n",
        "def evaluate(data_loader):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    total_loss_sen = 0\n",
        "\n",
        "\n",
        "    #loss\n",
        "    with torch.no_grad():\n",
        "        for iteration, sample in enumerate(data_loader):\n",
        "            d1=Variable(sample['d1'])\n",
        "            d1_fp = Variable(sample['d1_fp'].float())\n",
        "            d1_sm = Variable(sample['d1_sm'])\n",
        "            d1_gn = Variable(sample['d1_gn'].float())\n",
        "\n",
        "            d2=Variable(sample['d2'])\n",
        "            d2_fp = Variable(sample['d2_fp'].float())\n",
        "            d2_sm = Variable(sample['d2_sm'])\n",
        "            d2_gn = Variable(sample['d2_gn'].float())\n",
        "\n",
        "            cell = Variable(sample['cell'])\n",
        "            c_ts = Variable(sample['c_ts'])\n",
        "            c_ds = Variable(sample['c_ds'])\n",
        "            c_gn = Variable(sample['c_gn'].float())\n",
        "\n",
        "            syn_true = Variable(sample['syn'].float())\n",
        "            ri_d1=Variable(sample['ri_d1'].float())\n",
        "            ri_d2=Variable(sample['ri_d2'].float())\n",
        "\n",
        "\n",
        "            if cuda:\n",
        "                d1=d1.cuda()\n",
        "                d1_fp=d1_fp.cuda()\n",
        "                d1_sm=d1_sm.cuda()\n",
        "                d1_gn=d1_gn.cuda()\n",
        "\n",
        "                d2=d2.cuda()\n",
        "                d2_fp=d2_fp.cuda()\n",
        "                d2_sm=d2_sm.cuda()\n",
        "                d2_gn=d2_gn.cuda()\n",
        "\n",
        "                cell=cell.cuda()\n",
        "                c_ts=c_ts.cuda()\n",
        "                c_ds=c_ds.cuda()\n",
        "                c_gn=c_gn.cuda()\n",
        "\n",
        "                syn_true=syn_true.cuda()\n",
        "                ri_d1=ri_d1.cuda()\n",
        "                ri_d2=ri_d2.cuda()\n",
        "\n",
        "\n",
        "            syn,ri1,ri2 = model((d1, d1_fp, d1_sm, d1_gn), (d2, d2_fp, d2_sm, d2_gn), (cell, c_ts,c_ds,c_gn) )\n",
        "            loss = criterion_bce(syn, syn_true.view(-1,1))\n",
        "            total_loss +=loss.data\n",
        "            loss_sen = (criterion_bce(ri1, ri_d1.view(-1,1))+criterion_bce(ri2, ri_d2.view(-1,1)))/2\n",
        "            total_loss_sen += loss_sen.data\n",
        "\n",
        "        print('syn mse', total_loss.item()/(iteration+1))\n",
        "        print('sen_mse', total_loss_sen.item()/(iteration+1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MgE71MwjTwBH",
        "outputId": "0c15448b-9a6e-484f-d8ae-6ecb6a90b535"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch   1 |   100/   17 batches | ms/batch 90.82 | loss  0.37345\n",
            "| epoch   1 |   200/   17 batches | ms/batch 90.16 | loss  0.32039\n",
            "| epoch   1 |   300/   17 batches | ms/batch 84.83 | loss  0.30158\n",
            "| epoch   1 |   400/   17 batches | ms/batch 89.44 | loss  0.29340\n",
            "| epoch   1 |   500/   17 batches | ms/batch 89.39 | loss  0.28483\n",
            "| epoch   1 |   600/   17 batches | ms/batch 86.29 | loss  0.29483\n",
            "| epoch   1 |   700/   17 batches | ms/batch 89.03 | loss  0.27432\n",
            "| epoch   1 |   800/   17 batches | ms/batch 88.85 | loss  0.27499\n",
            "| epoch   1 |   900/   17 batches | ms/batch 84.80 | loss  0.27606\n",
            "| epoch   1 |  1000/   17 batches | ms/batch 90.83 | loss  0.25913\n",
            "| epoch   1 |  1100/   17 batches | ms/batch 106.84 | loss  0.27614\n",
            "| epoch   1 |  1200/   17 batches | ms/batch 89.53 | loss  0.26944\n",
            "| epoch   1 |  1300/   17 batches | ms/batch 88.52 | loss  0.26507\n",
            "| epoch   1 |  1400/   17 batches | ms/batch 87.24 | loss  0.25955\n",
            "| epoch   1 |  1500/   17 batches | ms/batch 85.31 | loss  0.26674\n",
            "| epoch   1 |  1600/   17 batches | ms/batch 86.15 | loss  0.25302\n",
            "| epoch   1 |  1700/   17 batches | ms/batch 89.13 | loss  0.25105\n",
            "| epoch   1 |  1800/   17 batches | ms/batch 85.88 | loss  0.25979\n",
            "| epoch   1 |  1900/   17 batches | ms/batch 88.69 | loss  0.26158\n",
            "| epoch   1 |  2000/   17 batches | ms/batch 88.73 | loss  0.25020\n",
            "| epoch   1 |  2100/   17 batches | ms/batch 85.03 | loss  0.25215\n",
            "| epoch   1 |  2200/   17 batches | ms/batch 88.87 | loss  0.25493\n",
            "| epoch   1 |   100/   17 batches | ms/batch 89.23 | loss  0.25521\n",
            "| epoch   1 |   200/   17 batches | ms/batch 88.87 | loss  0.19739\n",
            "| epoch   1 |   300/   17 batches | ms/batch 87.29 | loss  0.19561\n",
            "| epoch   1 |   400/   17 batches | ms/batch 89.25 | loss  0.19333\n",
            "| epoch   1 |   500/   17 batches | ms/batch 88.99 | loss  0.19156\n",
            "| epoch   1 |   600/   17 batches | ms/batch 85.56 | loss  0.19176\n",
            "| epoch   1 |   700/   17 batches | ms/batch 90.17 | loss  0.19115\n",
            "| epoch   1 |   800/   17 batches | ms/batch 92.52 | loss  0.19478\n",
            "| epoch   1 |   900/   17 batches | ms/batch 85.34 | loss  0.19162\n",
            "| epoch   1 |  1000/   17 batches | ms/batch 111.83 | loss  0.18614\n",
            "| epoch   1 |  1100/   17 batches | ms/batch 91.03 | loss  0.17870\n",
            "| epoch   1 |  1200/   17 batches | ms/batch 86.85 | loss  0.19047\n",
            "| epoch   1 |  1300/   17 batches | ms/batch 88.06 | loss  0.18680\n",
            "| epoch   1 |  1400/   17 batches | ms/batch 89.63 | loss  0.18133\n",
            "| epoch   1 |  1500/   17 batches | ms/batch 87.01 | loss  0.18589\n",
            "| epoch   1 |  1600/   17 batches | ms/batch 88.20 | loss  0.17816\n",
            "| epoch   1 |  1700/   17 batches | ms/batch 89.07 | loss  0.18036\n",
            "| epoch   1 |  1800/   17 batches | ms/batch 86.36 | loss  0.18896\n",
            "| epoch   1 |  1900/   17 batches | ms/batch 91.01 | loss  0.18275\n",
            "| epoch   1 |  2000/   17 batches | ms/batch 88.88 | loss  0.18672\n",
            "| epoch   1 |  2100/   17 batches | ms/batch 85.36 | loss  0.18494\n",
            "| epoch   1 |  2200/   17 batches | ms/batch 88.74 | loss  0.19452\n",
            "syn mse 0.24762109866701784\n",
            "sen_mse 0.0860751898610237\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   2 |   100/   17 batches | ms/batch 90.47 | loss  0.27419\n",
            "| epoch   2 |   200/   17 batches | ms/batch 85.86 | loss  0.25088\n",
            "| epoch   2 |   300/   17 batches | ms/batch 89.00 | loss  0.24562\n",
            "| epoch   2 |   400/   17 batches | ms/batch 89.15 | loss  0.24429\n",
            "| epoch   2 |   500/   17 batches | ms/batch 84.98 | loss  0.25212\n",
            "| epoch   2 |   600/   17 batches | ms/batch 89.03 | loss  0.24192\n",
            "| epoch   2 |   700/   17 batches | ms/batch 87.52 | loss  0.24648\n",
            "| epoch   2 |   800/   17 batches | ms/batch 87.26 | loss  0.23390\n",
            "| epoch   2 |   900/   17 batches | ms/batch 114.80 | loss  0.24074\n",
            "| epoch   2 |  1000/   17 batches | ms/batch 88.69 | loss  0.23923\n",
            "| epoch   2 |  1100/   17 batches | ms/batch 85.55 | loss  0.23832\n",
            "| epoch   2 |  1200/   17 batches | ms/batch 89.15 | loss  0.24623\n",
            "| epoch   2 |  1300/   17 batches | ms/batch 88.62 | loss  0.24185\n",
            "| epoch   2 |  1400/   17 batches | ms/batch 84.98 | loss  0.24383\n",
            "| epoch   2 |  1500/   17 batches | ms/batch 88.76 | loss  0.24177\n",
            "| epoch   2 |  1600/   17 batches | ms/batch 87.74 | loss  0.23688\n",
            "| epoch   2 |  1700/   17 batches | ms/batch 84.92 | loss  0.23464\n",
            "| epoch   2 |  1800/   17 batches | ms/batch 102.03 | loss  0.23436\n",
            "| epoch   2 |  1900/   17 batches | ms/batch 86.39 | loss  0.23983\n",
            "| epoch   2 |  2000/   17 batches | ms/batch 87.29 | loss  0.23810\n",
            "| epoch   2 |  2100/   17 batches | ms/batch 88.81 | loss  0.23593\n",
            "| epoch   2 |  2200/   17 batches | ms/batch 86.11 | loss  0.23512\n",
            "| epoch   2 |   100/   17 batches | ms/batch 91.43 | loss  0.18588\n",
            "| epoch   2 |   200/   17 batches | ms/batch 85.01 | loss  0.17566\n",
            "| epoch   2 |   300/   17 batches | ms/batch 89.56 | loss  0.19378\n",
            "| epoch   2 |   400/   17 batches | ms/batch 89.42 | loss  0.19098\n",
            "| epoch   2 |   500/   17 batches | ms/batch 84.84 | loss  0.18725\n",
            "| epoch   2 |   600/   17 batches | ms/batch 88.98 | loss  0.19255\n",
            "| epoch   2 |   700/   17 batches | ms/batch 87.96 | loss  0.17754\n",
            "| epoch   2 |   800/   17 batches | ms/batch 86.83 | loss  0.18681\n",
            "| epoch   2 |   900/   17 batches | ms/batch 89.19 | loss  0.17715\n",
            "| epoch   2 |  1000/   17 batches | ms/batch 85.19 | loss  0.18793\n",
            "| epoch   2 |  1100/   17 batches | ms/batch 85.98 | loss  0.19478\n",
            "| epoch   2 |  1200/   17 batches | ms/batch 86.15 | loss  0.18620\n",
            "| epoch   2 |  1300/   17 batches | ms/batch 93.71 | loss  0.16920\n",
            "| epoch   2 |  1400/   17 batches | ms/batch 87.61 | loss  0.17446\n",
            "| epoch   2 |  1500/   17 batches | ms/batch 88.82 | loss  0.17335\n",
            "| epoch   2 |  1600/   17 batches | ms/batch 87.31 | loss  0.18256\n",
            "| epoch   2 |  1700/   17 batches | ms/batch 88.55 | loss  0.17498\n",
            "| epoch   2 |  1800/   17 batches | ms/batch 88.84 | loss  0.18416\n",
            "| epoch   2 |  1900/   17 batches | ms/batch 120.57 | loss  0.17417\n",
            "| epoch   2 |  2000/   17 batches | ms/batch 84.76 | loss  0.18643\n",
            "| epoch   2 |  2100/   17 batches | ms/batch 88.77 | loss  0.16985\n",
            "| epoch   2 |  2200/   17 batches | ms/batch 89.29 | loss  0.17151\n",
            "syn mse 0.22192410871569204\n",
            "sen_mse 0.08636473941301924\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   3 |   100/   17 batches | ms/batch 86.26 | loss  0.24758\n",
            "| epoch   3 |   200/   17 batches | ms/batch 89.14 | loss  0.23346\n",
            "| epoch   3 |   300/   17 batches | ms/batch 89.79 | loss  0.23270\n",
            "| epoch   3 |   400/   17 batches | ms/batch 91.91 | loss  0.22875\n",
            "| epoch   3 |   500/   17 batches | ms/batch 88.77 | loss  0.23404\n",
            "| epoch   3 |   600/   17 batches | ms/batch 88.68 | loss  0.22904\n",
            "| epoch   3 |   700/   17 batches | ms/batch 84.74 | loss  0.24010\n",
            "| epoch   3 |   800/   17 batches | ms/batch 88.33 | loss  0.23695\n",
            "| epoch   3 |   900/   17 batches | ms/batch 87.95 | loss  0.22994\n",
            "| epoch   3 |  1000/   17 batches | ms/batch 85.40 | loss  0.22902\n",
            "| epoch   3 |  1100/   17 batches | ms/batch 88.57 | loss  0.23278\n",
            "| epoch   3 |  1200/   17 batches | ms/batch 88.44 | loss  0.22900\n",
            "| epoch   3 |  1300/   17 batches | ms/batch 86.90 | loss  0.22949\n",
            "| epoch   3 |  1400/   17 batches | ms/batch 89.04 | loss  0.22361\n",
            "| epoch   3 |  1500/   17 batches | ms/batch 86.31 | loss  0.23583\n",
            "| epoch   3 |  1600/   17 batches | ms/batch 87.67 | loss  0.23086\n",
            "| epoch   3 |  1700/   17 batches | ms/batch 88.90 | loss  0.22607\n",
            "| epoch   3 |  1800/   17 batches | ms/batch 84.93 | loss  0.22763\n",
            "| epoch   3 |  1900/   17 batches | ms/batch 85.93 | loss  0.22923\n",
            "| epoch   3 |  2000/   17 batches | ms/batch 91.91 | loss  0.22784\n",
            "| epoch   3 |  2100/   17 batches | ms/batch 84.82 | loss  0.22724\n",
            "| epoch   3 |  2200/   17 batches | ms/batch 88.16 | loss  0.22513\n",
            "| epoch   3 |   100/   17 batches | ms/batch 92.22 | loss  0.17491\n",
            "| epoch   3 |   200/   17 batches | ms/batch 88.66 | loss  0.18266\n",
            "| epoch   3 |   300/   17 batches | ms/batch 86.88 | loss  0.18339\n",
            "| epoch   3 |   400/   17 batches | ms/batch 87.51 | loss  0.17298\n",
            "| epoch   3 |   500/   17 batches | ms/batch 91.41 | loss  0.18649\n",
            "| epoch   3 |   600/   17 batches | ms/batch 85.57 | loss  0.16899\n",
            "| epoch   3 |   700/   17 batches | ms/batch 88.72 | loss  0.18898\n",
            "| epoch   3 |   800/   17 batches | ms/batch 89.10 | loss  0.18108\n",
            "| epoch   3 |   900/   17 batches | ms/batch 124.27 | loss  0.18111\n",
            "| epoch   3 |  1000/   17 batches | ms/batch 84.99 | loss  0.18271\n",
            "| epoch   3 |  1100/   17 batches | ms/batch 89.13 | loss  0.18572\n",
            "| epoch   3 |  1200/   17 batches | ms/batch 88.50 | loss  0.17118\n",
            "| epoch   3 |  1300/   17 batches | ms/batch 86.86 | loss  0.17241\n",
            "| epoch   3 |  1400/   17 batches | ms/batch 88.72 | loss  0.16890\n",
            "| epoch   3 |  1500/   17 batches | ms/batch 86.36 | loss  0.17620\n",
            "| epoch   3 |  1600/   17 batches | ms/batch 87.30 | loss  0.17539\n",
            "| epoch   3 |  1700/   17 batches | ms/batch 89.43 | loss  0.17477\n",
            "| epoch   3 |  1800/   17 batches | ms/batch 86.86 | loss  0.17868\n",
            "| epoch   3 |  1900/   17 batches | ms/batch 96.74 | loss  0.17409\n",
            "| epoch   3 |  2000/   17 batches | ms/batch 91.57 | loss  0.17764\n",
            "| epoch   3 |  2100/   17 batches | ms/batch 84.97 | loss  0.16828\n",
            "| epoch   3 |  2200/   17 batches | ms/batch 88.47 | loss  0.16820\n",
            "syn mse 0.21511625700781933\n",
            "sen_mse 0.08463449695273163\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   4 |   100/   17 batches | ms/batch 89.80 | loss  0.22893\n",
            "| epoch   4 |   200/   17 batches | ms/batch 87.78 | loss  0.22822\n",
            "| epoch   4 |   300/   17 batches | ms/batch 86.39 | loss  0.22380\n",
            "| epoch   4 |   400/   17 batches | ms/batch 86.52 | loss  0.22542\n",
            "| epoch   4 |   500/   17 batches | ms/batch 87.77 | loss  0.22701\n",
            "| epoch   4 |   600/   17 batches | ms/batch 88.47 | loss  0.23173\n",
            "| epoch   4 |   700/   17 batches | ms/batch 85.28 | loss  0.22101\n",
            "| epoch   4 |   800/   17 batches | ms/batch 88.34 | loss  0.21876\n",
            "| epoch   4 |   900/   17 batches | ms/batch 90.36 | loss  0.23108\n",
            "| epoch   4 |  1000/   17 batches | ms/batch 89.79 | loss  0.22141\n",
            "| epoch   4 |  1100/   17 batches | ms/batch 89.49 | loss  0.21794\n",
            "| epoch   4 |  1200/   17 batches | ms/batch 88.60 | loss  0.22576\n",
            "| epoch   4 |  1300/   17 batches | ms/batch 85.10 | loss  0.22542\n",
            "| epoch   4 |  1400/   17 batches | ms/batch 88.45 | loss  0.22843\n",
            "| epoch   4 |  1500/   17 batches | ms/batch 89.01 | loss  0.21247\n",
            "| epoch   4 |  1600/   17 batches | ms/batch 85.10 | loss  0.22265\n",
            "| epoch   4 |  1700/   17 batches | ms/batch 90.57 | loss  0.22144\n",
            "| epoch   4 |  1800/   17 batches | ms/batch 87.90 | loss  0.21101\n",
            "| epoch   4 |  1900/   17 batches | ms/batch 85.77 | loss  0.21900\n",
            "| epoch   4 |  2000/   17 batches | ms/batch 86.63 | loss  0.22178\n",
            "| epoch   4 |  2100/   17 batches | ms/batch 86.69 | loss  0.22514\n",
            "| epoch   4 |  2200/   17 batches | ms/batch 87.34 | loss  0.22214\n",
            "| epoch   4 |   100/   17 batches | ms/batch 86.12 | loss  0.17531\n",
            "| epoch   4 |   200/   17 batches | ms/batch 92.29 | loss  0.17434\n",
            "| epoch   4 |   300/   17 batches | ms/batch 88.94 | loss  0.17664\n",
            "| epoch   4 |   400/   17 batches | ms/batch 85.02 | loss  0.17885\n",
            "| epoch   4 |   500/   17 batches | ms/batch 95.64 | loss  0.18009\n",
            "| epoch   4 |   600/   17 batches | ms/batch 88.52 | loss  0.17440\n",
            "| epoch   4 |   700/   17 batches | ms/batch 158.34 | loss  0.16528\n",
            "| epoch   4 |   800/   17 batches | ms/batch 87.43 | loss  0.17713\n",
            "| epoch   4 |   900/   17 batches | ms/batch 87.74 | loss  0.17340\n",
            "| epoch   4 |  1000/   17 batches | ms/batch 89.21 | loss  0.16996\n",
            "| epoch   4 |  1100/   17 batches | ms/batch 87.44 | loss  0.16924\n",
            "| epoch   4 |  1200/   17 batches | ms/batch 88.06 | loss  0.17373\n",
            "| epoch   4 |  1300/   17 batches | ms/batch 86.50 | loss  0.17844\n",
            "| epoch   4 |  1400/   17 batches | ms/batch 84.90 | loss  0.16419\n",
            "| epoch   4 |  1500/   17 batches | ms/batch 87.50 | loss  0.18112\n",
            "| epoch   4 |  1600/   17 batches | ms/batch 90.03 | loss  0.17769\n",
            "| epoch   4 |  1700/   17 batches | ms/batch 85.34 | loss  0.16519\n",
            "| epoch   4 |  1800/   17 batches | ms/batch 88.96 | loss  0.16982\n",
            "| epoch   4 |  1900/   17 batches | ms/batch 89.04 | loss  0.16417\n",
            "| epoch   4 |  2000/   17 batches | ms/batch 85.20 | loss  0.17039\n",
            "| epoch   4 |  2100/   17 batches | ms/batch 89.24 | loss  0.15739\n",
            "| epoch   4 |  2200/   17 batches | ms/batch 99.26 | loss  0.16705\n",
            "syn mse 0.2059281143749658\n",
            "sen_mse 0.08394957507345598\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   5 |   100/   17 batches | ms/batch 86.48 | loss  0.23197\n",
            "| epoch   5 |   200/   17 batches | ms/batch 89.68 | loss  0.22511\n",
            "| epoch   5 |   300/   17 batches | ms/batch 88.61 | loss  0.22449\n",
            "| epoch   5 |   400/   17 batches | ms/batch 86.21 | loss  0.22578\n",
            "| epoch   5 |   500/   17 batches | ms/batch 88.86 | loss  0.21709\n",
            "| epoch   5 |   600/   17 batches | ms/batch 87.36 | loss  0.21564\n",
            "| epoch   5 |   700/   17 batches | ms/batch 85.92 | loss  0.21465\n",
            "| epoch   5 |   800/   17 batches | ms/batch 88.48 | loss  0.21103\n",
            "| epoch   5 |   900/   17 batches | ms/batch 86.16 | loss  0.22269\n",
            "| epoch   5 |  1000/   17 batches | ms/batch 87.29 | loss  0.22461\n",
            "| epoch   5 |  1100/   17 batches | ms/batch 88.57 | loss  0.21009\n",
            "| epoch   5 |  1200/   17 batches | ms/batch 88.13 | loss  0.21783\n",
            "| epoch   5 |  1300/   17 batches | ms/batch 92.37 | loss  0.21370\n",
            "| epoch   5 |  1400/   17 batches | ms/batch 89.09 | loss  0.21299\n",
            "| epoch   5 |  1500/   17 batches | ms/batch 86.09 | loss  0.20976\n",
            "| epoch   5 |  1600/   17 batches | ms/batch 87.61 | loss  0.21450\n",
            "| epoch   5 |  1700/   17 batches | ms/batch 88.56 | loss  0.20617\n",
            "| epoch   5 |  1800/   17 batches | ms/batch 85.43 | loss  0.22199\n",
            "| epoch   5 |  1900/   17 batches | ms/batch 90.03 | loss  0.21811\n",
            "| epoch   5 |  2000/   17 batches | ms/batch 86.25 | loss  0.21045\n",
            "| epoch   5 |  2100/   17 batches | ms/batch 84.52 | loss  0.20493\n",
            "| epoch   5 |  2200/   17 batches | ms/batch 86.43 | loss  0.20477\n",
            "| epoch   5 |   100/   17 batches | ms/batch 87.74 | loss  0.17295\n",
            "| epoch   5 |   200/   17 batches | ms/batch 88.62 | loss  0.17773\n",
            "| epoch   5 |   300/   17 batches | ms/batch 87.02 | loss  0.16915\n",
            "| epoch   5 |   400/   17 batches | ms/batch 88.46 | loss  0.17261\n",
            "| epoch   5 |   500/   17 batches | ms/batch 88.48 | loss  0.17353\n",
            "| epoch   5 |   600/   17 batches | ms/batch 86.40 | loss  0.17052\n",
            "| epoch   5 |   700/   17 batches | ms/batch 88.01 | loss  0.17196\n",
            "| epoch   5 |   800/   17 batches | ms/batch 100.41 | loss  0.16622\n",
            "| epoch   5 |   900/   17 batches | ms/batch 85.19 | loss  0.17239\n",
            "| epoch   5 |  1000/   17 batches | ms/batch 89.48 | loss  0.17228\n",
            "| epoch   5 |  1100/   17 batches | ms/batch 91.35 | loss  0.16575\n",
            "| epoch   5 |  1200/   17 batches | ms/batch 85.07 | loss  0.16501\n",
            "| epoch   5 |  1300/   17 batches | ms/batch 89.62 | loss  0.17302\n",
            "| epoch   5 |  1400/   17 batches | ms/batch 88.46 | loss  0.16168\n",
            "| epoch   5 |  1500/   17 batches | ms/batch 85.47 | loss  0.17039\n",
            "| epoch   5 |  1600/   17 batches | ms/batch 89.08 | loss  0.16898\n",
            "| epoch   5 |  1700/   17 batches | ms/batch 174.20 | loss  0.16649\n",
            "| epoch   5 |  1800/   17 batches | ms/batch 90.49 | loss  0.16146\n",
            "| epoch   5 |  1900/   17 batches | ms/batch 87.61 | loss  0.16127\n",
            "| epoch   5 |  2000/   17 batches | ms/batch 87.99 | loss  0.16093\n",
            "| epoch   5 |  2100/   17 batches | ms/batch 88.96 | loss  0.16239\n",
            "| epoch   5 |  2200/   17 batches | ms/batch 86.73 | loss  0.16720\n",
            "syn mse 0.2079108350122455\n",
            "sen_mse 0.08417936548877723\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   6 |   100/   17 batches | ms/batch 86.07 | loss  0.23589\n",
            "| epoch   6 |   200/   17 batches | ms/batch 88.72 | loss  0.21773\n",
            "| epoch   6 |   300/   17 batches | ms/batch 89.78 | loss  0.21690\n",
            "| epoch   6 |   400/   17 batches | ms/batch 85.77 | loss  0.21791\n",
            "| epoch   6 |   500/   17 batches | ms/batch 86.77 | loss  0.20779\n",
            "| epoch   6 |   600/   17 batches | ms/batch 88.25 | loss  0.21935\n",
            "| epoch   6 |   700/   17 batches | ms/batch 85.77 | loss  0.21337\n",
            "| epoch   6 |   800/   17 batches | ms/batch 88.23 | loss  0.21035\n",
            "| epoch   6 |   900/   17 batches | ms/batch 85.99 | loss  0.20702\n",
            "| epoch   6 |  1000/   17 batches | ms/batch 86.83 | loss  0.21494\n",
            "| epoch   6 |  1100/   17 batches | ms/batch 87.90 | loss  0.21630\n",
            "| epoch   6 |  1200/   17 batches | ms/batch 86.20 | loss  0.20955\n",
            "| epoch   6 |  1300/   17 batches | ms/batch 89.40 | loss  0.21396\n",
            "| epoch   6 |  1400/   17 batches | ms/batch 88.03 | loss  0.20370\n",
            "| epoch   6 |  1500/   17 batches | ms/batch 90.44 | loss  0.20911\n",
            "| epoch   6 |  1600/   17 batches | ms/batch 87.93 | loss  0.20918\n",
            "| epoch   6 |  1700/   17 batches | ms/batch 88.97 | loss  0.21037\n",
            "| epoch   6 |  1800/   17 batches | ms/batch 85.11 | loss  0.20367\n",
            "| epoch   6 |  1900/   17 batches | ms/batch 88.32 | loss  0.20524\n",
            "| epoch   6 |  2000/   17 batches | ms/batch 88.01 | loss  0.20445\n",
            "| epoch   6 |  2100/   17 batches | ms/batch 86.46 | loss  0.20789\n",
            "| epoch   6 |  2200/   17 batches | ms/batch 88.66 | loss  0.20930\n",
            "| epoch   6 |   100/   17 batches | ms/batch 88.17 | loss  0.17667\n",
            "| epoch   6 |   200/   17 batches | ms/batch 88.92 | loss  0.16380\n",
            "| epoch   6 |   300/   17 batches | ms/batch 86.42 | loss  0.18059\n",
            "| epoch   6 |   400/   17 batches | ms/batch 87.26 | loss  0.16649\n",
            "| epoch   6 |   500/   17 batches | ms/batch 89.59 | loss  0.16756\n",
            "| epoch   6 |   600/   17 batches | ms/batch 85.98 | loss  0.16998\n",
            "| epoch   6 |   700/   17 batches | ms/batch 87.85 | loss  0.16845\n",
            "| epoch   6 |   800/   17 batches | ms/batch 88.75 | loss  0.17205\n",
            "| epoch   6 |   900/   17 batches | ms/batch 85.57 | loss  0.16229\n",
            "| epoch   6 |  1000/   17 batches | ms/batch 100.31 | loss  0.17482\n",
            "| epoch   6 |  1100/   17 batches | ms/batch 89.00 | loss  0.16390\n",
            "| epoch   6 |  1200/   17 batches | ms/batch 84.84 | loss  0.15823\n",
            "| epoch   6 |  1300/   17 batches | ms/batch 91.49 | loss  0.16368\n",
            "| epoch   6 |  1400/   17 batches | ms/batch 88.91 | loss  0.16049\n",
            "| epoch   6 |  1500/   17 batches | ms/batch 84.80 | loss  0.15994\n",
            "| epoch   6 |  1600/   17 batches | ms/batch 85.96 | loss  0.16280\n",
            "| epoch   6 |  1700/   17 batches | ms/batch 85.66 | loss  0.16323\n",
            "| epoch   6 |  1800/   17 batches | ms/batch 85.79 | loss  0.16545\n",
            "| epoch   6 |  1900/   17 batches | ms/batch 88.91 | loss  0.15469\n",
            "| epoch   6 |  2000/   17 batches | ms/batch 88.67 | loss  0.15906\n",
            "| epoch   6 |  2100/   17 batches | ms/batch 86.68 | loss  0.16670\n",
            "| epoch   6 |  2200/   17 batches | ms/batch 90.10 | loss  0.15713\n",
            "syn mse 0.2047107783382703\n",
            "sen_mse 0.08493331267829535\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   7 |   100/   17 batches | ms/batch 90.99 | loss  0.21915\n",
            "| epoch   7 |   200/   17 batches | ms/batch 86.00 | loss  0.22338\n",
            "| epoch   7 |   300/   17 batches | ms/batch 89.40 | loss  0.20905\n",
            "| epoch   7 |   400/   17 batches | ms/batch 89.49 | loss  0.21005\n",
            "| epoch   7 |   500/   17 batches | ms/batch 85.02 | loss  0.20920\n",
            "| epoch   7 |   600/   17 batches | ms/batch 88.35 | loss  0.20930\n",
            "| epoch   7 |   700/   17 batches | ms/batch 86.66 | loss  0.20370\n",
            "| epoch   7 |   800/   17 batches | ms/batch 87.09 | loss  0.19233\n",
            "| epoch   7 |   900/   17 batches | ms/batch 88.89 | loss  0.21415\n",
            "| epoch   7 |  1000/   17 batches | ms/batch 86.70 | loss  0.20003\n",
            "| epoch   7 |  1100/   17 batches | ms/batch 87.15 | loss  0.20618\n",
            "| epoch   7 |  1200/   17 batches | ms/batch 185.70 | loss  0.20738\n",
            "| epoch   7 |  1300/   17 batches | ms/batch 86.97 | loss  0.21122\n",
            "| epoch   7 |  1400/   17 batches | ms/batch 91.59 | loss  0.20435\n",
            "| epoch   7 |  1500/   17 batches | ms/batch 87.07 | loss  0.21727\n",
            "| epoch   7 |  1600/   17 batches | ms/batch 89.89 | loss  0.21273\n",
            "| epoch   7 |  1700/   17 batches | ms/batch 92.06 | loss  0.21232\n",
            "| epoch   7 |  1800/   17 batches | ms/batch 86.06 | loss  0.20890\n",
            "| epoch   7 |  1900/   17 batches | ms/batch 87.49 | loss  0.21375\n",
            "| epoch   7 |  2000/   17 batches | ms/batch 88.83 | loss  0.20580\n",
            "| epoch   7 |  2100/   17 batches | ms/batch 86.31 | loss  0.20328\n",
            "| epoch   7 |  2200/   17 batches | ms/batch 88.51 | loss  0.20216\n",
            "| epoch   7 |   100/   17 batches | ms/batch 85.98 | loss  0.16976\n",
            "| epoch   7 |   200/   17 batches | ms/batch 88.68 | loss  0.16277\n",
            "| epoch   7 |   300/   17 batches | ms/batch 88.07 | loss  0.17407\n",
            "| epoch   7 |   400/   17 batches | ms/batch 86.60 | loss  0.16108\n",
            "| epoch   7 |   500/   17 batches | ms/batch 88.43 | loss  0.15259\n",
            "| epoch   7 |   600/   17 batches | ms/batch 88.66 | loss  0.16216\n",
            "| epoch   7 |   700/   17 batches | ms/batch 87.43 | loss  0.17377\n",
            "| epoch   7 |   800/   17 batches | ms/batch 88.84 | loss  0.16462\n",
            "| epoch   7 |   900/   17 batches | ms/batch 86.55 | loss  0.16210\n",
            "| epoch   7 |  1000/   17 batches | ms/batch 87.29 | loss  0.15881\n",
            "| epoch   7 |  1100/   17 batches | ms/batch 99.72 | loss  0.16792\n",
            "| epoch   7 |  1200/   17 batches | ms/batch 85.59 | loss  0.15631\n",
            "| epoch   7 |  1300/   17 batches | ms/batch 89.96 | loss  0.16230\n",
            "| epoch   7 |  1400/   17 batches | ms/batch 88.34 | loss  0.16158\n",
            "| epoch   7 |  1500/   17 batches | ms/batch 85.09 | loss  0.15565\n",
            "| epoch   7 |  1600/   17 batches | ms/batch 89.06 | loss  0.16822\n",
            "| epoch   7 |  1700/   17 batches | ms/batch 87.07 | loss  0.15735\n",
            "| epoch   7 |  1800/   17 batches | ms/batch 85.19 | loss  0.16174\n",
            "| epoch   7 |  1900/   17 batches | ms/batch 88.42 | loss  0.15687\n",
            "| epoch   7 |  2000/   17 batches | ms/batch 92.29 | loss  0.16774\n",
            "| epoch   7 |  2100/   17 batches | ms/batch 85.14 | loss  0.15070\n",
            "| epoch   7 |  2200/   17 batches | ms/batch 88.68 | loss  0.15143\n",
            "syn mse 0.19395764780128064\n",
            "sen_mse 0.08446414499900805\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   8 |   100/   17 batches | ms/batch 94.29 | loss  0.22550\n",
            "| epoch   8 |   200/   17 batches | ms/batch 93.24 | loss  0.21143\n",
            "| epoch   8 |   300/   17 batches | ms/batch 89.14 | loss  0.20792\n",
            "| epoch   8 |   400/   17 batches | ms/batch 85.42 | loss  0.21526\n",
            "| epoch   8 |   500/   17 batches | ms/batch 88.17 | loss  0.20177\n",
            "| epoch   8 |   600/   17 batches | ms/batch 88.41 | loss  0.20707\n",
            "| epoch   8 |   700/   17 batches | ms/batch 84.38 | loss  0.20584\n",
            "| epoch   8 |   800/   17 batches | ms/batch 86.07 | loss  0.21276\n",
            "| epoch   8 |   900/   17 batches | ms/batch 88.36 | loss  0.20632\n",
            "| epoch   8 |  1000/   17 batches | ms/batch 84.91 | loss  0.20236\n",
            "| epoch   8 |  1100/   17 batches | ms/batch 88.80 | loss  0.20656\n",
            "| epoch   8 |  1200/   17 batches | ms/batch 87.48 | loss  0.19932\n",
            "| epoch   8 |  1300/   17 batches | ms/batch 86.15 | loss  0.20800\n",
            "| epoch   8 |  1400/   17 batches | ms/batch 89.13 | loss  0.21005\n",
            "| epoch   8 |  1500/   17 batches | ms/batch 90.15 | loss  0.20391\n",
            "| epoch   8 |  1600/   17 batches | ms/batch 87.41 | loss  0.20172\n",
            "| epoch   8 |  1700/   17 batches | ms/batch 88.46 | loss  0.20507\n",
            "| epoch   8 |  1800/   17 batches | ms/batch 86.98 | loss  0.19650\n",
            "| epoch   8 |  1900/   17 batches | ms/batch 95.76 | loss  0.20121\n",
            "| epoch   8 |  2000/   17 batches | ms/batch 87.85 | loss  0.20382\n",
            "| epoch   8 |  2100/   17 batches | ms/batch 84.84 | loss  0.19724\n",
            "| epoch   8 |  2200/   17 batches | ms/batch 89.29 | loss  0.19956\n",
            "| epoch   8 |   100/   17 batches | ms/batch 88.95 | loss  0.17329\n",
            "| epoch   8 |   200/   17 batches | ms/batch 87.32 | loss  0.15844\n",
            "| epoch   8 |   300/   17 batches | ms/batch 85.96 | loss  0.16529\n",
            "| epoch   8 |   400/   17 batches | ms/batch 88.00 | loss  0.16821\n",
            "| epoch   8 |   500/   17 batches | ms/batch 89.71 | loss  0.16362\n",
            "| epoch   8 |   600/   17 batches | ms/batch 85.71 | loss  0.14998\n",
            "| epoch   8 |   700/   17 batches | ms/batch 89.36 | loss  0.16062\n",
            "| epoch   8 |   800/   17 batches | ms/batch 89.82 | loss  0.16259\n",
            "| epoch   8 |   900/   17 batches | ms/batch 85.21 | loss  0.15908\n",
            "| epoch   8 |  1000/   17 batches | ms/batch 88.98 | loss  0.15396\n",
            "| epoch   8 |  1100/   17 batches | ms/batch 88.47 | loss  0.15822\n",
            "| epoch   8 |  1200/   17 batches | ms/batch 86.18 | loss  0.16864\n",
            "| epoch   8 |  1300/   17 batches | ms/batch 94.50 | loss  0.15576\n",
            "| epoch   8 |  1400/   17 batches | ms/batch 90.31 | loss  0.15992\n",
            "| epoch   8 |  1500/   17 batches | ms/batch 84.97 | loss  0.14849\n",
            "| epoch   8 |  1600/   17 batches | ms/batch 88.81 | loss  0.15835\n",
            "| epoch   8 |  1700/   17 batches | ms/batch 88.21 | loss  0.15257\n",
            "| epoch   8 |  1800/   17 batches | ms/batch 85.69 | loss  0.16641\n",
            "| epoch   8 |  1900/   17 batches | ms/batch 86.69 | loss  0.15042\n",
            "| epoch   8 |  2000/   17 batches | ms/batch 85.11 | loss  0.15727\n",
            "| epoch   8 |  2100/   17 batches | ms/batch 87.23 | loss  0.15850\n",
            "| epoch   8 |  2200/   17 batches | ms/batch 88.83 | loss  0.16682\n",
            "syn mse 0.2082602773155306\n",
            "sen_mse 0.08656266274260138\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   9 |   100/   17 batches | ms/batch 199.01 | loss  0.22109\n",
            "| epoch   9 |   200/   17 batches | ms/batch 92.92 | loss  0.20910\n",
            "| epoch   9 |   300/   17 batches | ms/batch 89.16 | loss  0.21194\n",
            "| epoch   9 |   400/   17 batches | ms/batch 86.15 | loss  0.20465\n",
            "| epoch   9 |   500/   17 batches | ms/batch 88.36 | loss  0.20572\n",
            "| epoch   9 |   600/   17 batches | ms/batch 88.04 | loss  0.20716\n",
            "| epoch   9 |   700/   17 batches | ms/batch 85.64 | loss  0.20374\n",
            "| epoch   9 |   800/   17 batches | ms/batch 89.09 | loss  0.20079\n",
            "| epoch   9 |   900/   17 batches | ms/batch 86.40 | loss  0.20441\n",
            "| epoch   9 |  1000/   17 batches | ms/batch 86.11 | loss  0.20035\n",
            "| epoch   9 |  1100/   17 batches | ms/batch 88.01 | loss  0.19077\n",
            "| epoch   9 |  1200/   17 batches | ms/batch 86.54 | loss  0.20522\n",
            "| epoch   9 |  1300/   17 batches | ms/batch 87.69 | loss  0.20769\n",
            "| epoch   9 |  1400/   17 batches | ms/batch 91.96 | loss  0.20053\n",
            "| epoch   9 |  1500/   17 batches | ms/batch 85.43 | loss  0.20130\n",
            "| epoch   9 |  1600/   17 batches | ms/batch 87.07 | loss  0.19968\n",
            "| epoch   9 |  1700/   17 batches | ms/batch 87.69 | loss  0.19870\n",
            "| epoch   9 |  1800/   17 batches | ms/batch 89.03 | loss  0.20547\n",
            "| epoch   9 |  1900/   17 batches | ms/batch 87.27 | loss  0.19679\n",
            "| epoch   9 |  2000/   17 batches | ms/batch 87.55 | loss  0.19881\n",
            "| epoch   9 |  2100/   17 batches | ms/batch 86.30 | loss  0.20023\n",
            "| epoch   9 |  2200/   17 batches | ms/batch 87.53 | loss  0.19850\n",
            "| epoch   9 |   100/   17 batches | ms/batch 85.89 | loss  0.16628\n",
            "| epoch   9 |   200/   17 batches | ms/batch 88.17 | loss  0.16716\n",
            "| epoch   9 |   300/   17 batches | ms/batch 85.71 | loss  0.16477\n",
            "| epoch   9 |   400/   17 batches | ms/batch 84.67 | loss  0.15445\n",
            "| epoch   9 |   500/   17 batches | ms/batch 87.27 | loss  0.16735\n",
            "| epoch   9 |   600/   17 batches | ms/batch 87.90 | loss  0.15070\n",
            "| epoch   9 |   700/   17 batches | ms/batch 85.17 | loss  0.16041\n",
            "| epoch   9 |   800/   17 batches | ms/batch 87.77 | loss  0.15170\n",
            "| epoch   9 |   900/   17 batches | ms/batch 87.20 | loss  0.16187\n",
            "| epoch   9 |  1000/   17 batches | ms/batch 86.09 | loss  0.17312\n",
            "| epoch   9 |  1100/   17 batches | ms/batch 88.58 | loss  0.16491\n",
            "| epoch   9 |  1200/   17 batches | ms/batch 92.03 | loss  0.16646\n",
            "| epoch   9 |  1300/   17 batches | ms/batch 86.02 | loss  0.15831\n",
            "| epoch   9 |  1400/   17 batches | ms/batch 87.75 | loss  0.15201\n",
            "| epoch   9 |  1500/   17 batches | ms/batch 86.84 | loss  0.16027\n",
            "| epoch   9 |  1600/   17 batches | ms/batch 86.08 | loss  0.15829\n",
            "| epoch   9 |  1700/   17 batches | ms/batch 87.75 | loss  0.14804\n",
            "| epoch   9 |  1800/   17 batches | ms/batch 86.63 | loss  0.14767\n",
            "| epoch   9 |  1900/   17 batches | ms/batch 86.53 | loss  0.15081\n",
            "| epoch   9 |  2000/   17 batches | ms/batch 89.10 | loss  0.15256\n",
            "| epoch   9 |  2100/   17 batches | ms/batch 86.48 | loss  0.15400\n",
            "| epoch   9 |  2200/   17 batches | ms/batch 86.34 | loss  0.15238\n",
            "syn mse 0.19272924847444176\n",
            "sen_mse 0.08614719477718641\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  10 |   100/   17 batches | ms/batch 88.52 | loss  0.21972\n",
            "| epoch  10 |   200/   17 batches | ms/batch 88.86 | loss  0.20595\n",
            "| epoch  10 |   300/   17 batches | ms/batch 87.22 | loss  0.20463\n",
            "| epoch  10 |   400/   17 batches | ms/batch 87.82 | loss  0.20110\n",
            "| epoch  10 |   500/   17 batches | ms/batch 86.17 | loss  0.19963\n",
            "| epoch  10 |   600/   17 batches | ms/batch 86.49 | loss  0.20241\n",
            "| epoch  10 |   700/   17 batches | ms/batch 88.26 | loss  0.20498\n",
            "| epoch  10 |   800/   17 batches | ms/batch 85.61 | loss  0.20292\n",
            "| epoch  10 |   900/   17 batches | ms/batch 86.88 | loss  0.19328\n",
            "| epoch  10 |  1000/   17 batches | ms/batch 87.84 | loss  0.20297\n",
            "| epoch  10 |  1100/   17 batches | ms/batch 84.93 | loss  0.19569\n",
            "| epoch  10 |  1200/   17 batches | ms/batch 85.93 | loss  0.19453\n",
            "| epoch  10 |  1300/   17 batches | ms/batch 85.40 | loss  0.19682\n",
            "| epoch  10 |  1400/   17 batches | ms/batch 86.66 | loss  0.20029\n",
            "| epoch  10 |  1500/   17 batches | ms/batch 87.20 | loss  0.20568\n",
            "| epoch  10 |  1600/   17 batches | ms/batch 87.49 | loss  0.19614\n",
            "| epoch  10 |  1700/   17 batches | ms/batch 85.26 | loss  0.19466\n",
            "| epoch  10 |  1800/   17 batches | ms/batch 90.10 | loss  0.19179\n",
            "| epoch  10 |  1900/   17 batches | ms/batch 87.61 | loss  0.19984\n",
            "| epoch  10 |  2000/   17 batches | ms/batch 85.07 | loss  0.20182\n",
            "| epoch  10 |  2100/   17 batches | ms/batch 87.62 | loss  0.20299\n",
            "| epoch  10 |  2200/   17 batches | ms/batch 87.36 | loss  0.19482\n",
            "| epoch  10 |   100/   17 batches | ms/batch 88.66 | loss  0.16846\n",
            "| epoch  10 |   200/   17 batches | ms/batch 87.75 | loss  0.15746\n",
            "| epoch  10 |   300/   17 batches | ms/batch 85.02 | loss  0.15640\n",
            "| epoch  10 |   400/   17 batches | ms/batch 87.06 | loss  0.15613\n",
            "| epoch  10 |   500/   17 batches | ms/batch 87.61 | loss  0.16363\n",
            "| epoch  10 |   600/   17 batches | ms/batch 87.47 | loss  0.15594\n",
            "| epoch  10 |   700/   17 batches | ms/batch 86.38 | loss  0.15284\n",
            "| epoch  10 |   800/   17 batches | ms/batch 86.54 | loss  0.15269\n",
            "| epoch  10 |   900/   17 batches | ms/batch 86.33 | loss  0.15323\n",
            "| epoch  10 |  1000/   17 batches | ms/batch 87.26 | loss  0.16105\n",
            "| epoch  10 |  1100/   17 batches | ms/batch 86.42 | loss  0.15285\n",
            "| epoch  10 |  1200/   17 batches | ms/batch 90.74 | loss  0.16485\n",
            "| epoch  10 |  1300/   17 batches | ms/batch 88.21 | loss  0.15828\n",
            "| epoch  10 |  1400/   17 batches | ms/batch 86.29 | loss  0.15676\n",
            "| epoch  10 |  1500/   17 batches | ms/batch 86.13 | loss  0.15205\n",
            "| epoch  10 |  1600/   17 batches | ms/batch 87.10 | loss  0.15843\n",
            "| epoch  10 |  1700/   17 batches | ms/batch 85.68 | loss  0.16227\n",
            "| epoch  10 |  1800/   17 batches | ms/batch 86.43 | loss  0.15633\n",
            "| epoch  10 |  1900/   17 batches | ms/batch 87.53 | loss  0.14862\n",
            "| epoch  10 |  2000/   17 batches | ms/batch 86.15 | loss  0.15551\n",
            "| epoch  10 |  2100/   17 batches | ms/batch 86.24 | loss  0.15979\n",
            "| epoch  10 |  2200/   17 batches | ms/batch 87.78 | loss  0.15591\n",
            "syn mse 0.19367338270731857\n",
            "sen_mse 0.0862170752209665\n",
            "-----------------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "\n",
        "best_val_loss = None\n",
        "try:\n",
        "    for epoch in range(1, 11):\n",
        "        epoch_start_time = time.time()\n",
        "        training(False, train_loader)\n",
        "        training(True, train_loader)\n",
        "        evaluate(test_loader)\n",
        "        print('-'*89)\n",
        "except KeyboardInterrupt:\n",
        "    print('-'*89)\n",
        "    print('Existing from training early')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#model save\n",
        "#torch.save(model.state_dict(), data_path+'15-Pooled.p')\n",
        "#model load\n",
        "model.load_state_dict(torch.load(data_path+'15-Pooled.p'))\n",
        "#model.eval()"
      ],
      "metadata": {
        "id": "QMamMss01xfo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6305357-1921-4807-bd2c-eec8ffc5e79d"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oIQdW_5xTyeR"
      },
      "source": [
        "#Evaluate the general model\n",
        "The model is evaluated based on a few metrics the original paper uses the metrics as: AUROC, AUPRC\n",
        "\n",
        "The original paper achieved these results:\n",
        "\n",
        "{0.9577 AUROC, 0.8335 AUPRC} (General Model)\n",
        "\n",
        "Our implementation for this project will aim to hit these metrics. Any ablations we will also aim to hit these metrics."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Achieved Results"
      ],
      "metadata": {
        "id": "_v6oQwezU1YZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "VMak65z1T22l"
      },
      "outputs": [],
      "source": [
        "def dcg_score(y_score, y_true, k):\n",
        "    \"\"\"\n",
        "        https://www.kaggle.com/davidgasquez/ndcg-scorer\n",
        "        y_true: np.array, size= [n_samples]\n",
        "        y_score: np.array, size=[n_samples]\n",
        "        k: int, rank\n",
        "    \"\"\"\n",
        "    order = np.argsort(y_score)[::-1]\n",
        "    y_true = np.take(y_true, order[:k])\n",
        "\n",
        "    #gain = 2 ** y_true -1\n",
        "    gain = y_true\n",
        "\n",
        "    discounts = np.log2(np.arange(len(y_true)) + 2)\n",
        "    return np.sum(gain/discounts)\n",
        "\n",
        "def evaluate_accuracy(data_loader):\n",
        "    model.eval()\n",
        "\n",
        "    syn_all=[]\n",
        "    syn_true_all=[]\n",
        "    ri1_all=[]\n",
        "    ri1_true_all=[]\n",
        "    ri2_all=[]\n",
        "    ri2_true_all=[]\n",
        "\n",
        "    #loss\n",
        "    with torch.no_grad():\n",
        "        for iteration, sample in enumerate(data_loader):\n",
        "            d1=Variable(sample['d1'])\n",
        "            d1_fp = Variable(sample['d1_fp'].float())\n",
        "            d1_sm = Variable(sample['d1_sm'])\n",
        "            d1_gn = Variable(sample['d1_gn'].float())\n",
        "\n",
        "            d2=Variable(sample['d2'])\n",
        "            d2_fp = Variable(sample['d2_fp'].float())\n",
        "            d2_sm = Variable(sample['d2_sm'])\n",
        "            d2_gn = Variable(sample['d2_gn'].float())\n",
        "\n",
        "            cell = Variable(sample['cell'])\n",
        "            c_ts = Variable(sample['c_ts'])\n",
        "            c_ds = Variable(sample['c_ds'])\n",
        "            c_gn = Variable(sample['c_gn'].float())\n",
        "\n",
        "            syn_true = Variable(sample['syn'].float())\n",
        "            ri_d1=Variable(sample['ri_d1'])\n",
        "            ri_d2=Variable(sample['ri_d2'])\n",
        "\n",
        "            if cuda:\n",
        "                d1=d1.cuda()\n",
        "                d1_fp=d1_fp.cuda()\n",
        "                d1_sm=d1_sm.cuda()\n",
        "                d1_gn=d1_gn.cuda()\n",
        "\n",
        "                d2=d2.cuda()\n",
        "                d2_fp=d2_fp.cuda()\n",
        "                d2_sm=d2_sm.cuda()\n",
        "                d2_gn=d2_gn.cuda()\n",
        "\n",
        "                cell=cell.cuda()\n",
        "                c_ts=c_ts.cuda()\n",
        "                c_ds=c_ds.cuda()\n",
        "                c_gn=c_gn.cuda()\n",
        "\n",
        "\n",
        "            syn,ri1,ri2 = model((d1, d1_fp, d1_sm, d1_gn), (d2, d2_fp, d2_sm, d2_gn), (cell, c_ts,c_ds,c_gn) )\n",
        "\n",
        "            syn_all.append(syn.data.cpu().numpy())\n",
        "            syn_true_all.append(syn_true.numpy())\n",
        "\n",
        "            ri1_all.append(ri1.data.cpu().numpy())\n",
        "            ri1_true_all.append(ri_d1.numpy())\n",
        "\n",
        "            ri2_all.append(ri2.data.cpu().numpy())\n",
        "            ri2_true_all.append(ri_d2.numpy())\n",
        "\n",
        "    return syn_all, syn_true_all, ri1_all, ri1_true_all, ri2_all, ri2_true_all"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ChLmtEUUUUqb"
      },
      "source": [
        "## Evaluate synergy prediction (General Model)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "2Zn5cAD5USU-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff2d6d26-55ba-456a-dd89-1ecccd5be501"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.0\n",
            "0.7267833130011351\n",
            "0.9257503044782515\n"
          ]
        }
      ],
      "source": [
        "syn_all, syn_true_all, ri1_all, ri1_true_all, ri2_all, ri2_true_all= evaluate_accuracy(test_loader)\n",
        "\n",
        "syn_all= [s.item() for syn in syn_all for s in syn]\n",
        "syn_true_all = [s for syn in syn_true_all for s in syn]\n",
        "#NDCG\n",
        "print(dcg_score(syn_all,syn_true_all, k=20)/dcg_score(syn_true_all,syn_true_all, k=20))\n",
        "#AUPRC\n",
        "print(metrics.average_precision_score(syn_true_all,  1/(1 + np.exp(-np.array(syn_all)))))\n",
        "#AUROC\n",
        "print(metrics.roc_auc_score(syn_true_all,  1/(1 + np.exp(-np.array(syn_all)))))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0YP-RLwUcHG"
      },
      "source": [
        "## Evaluate sensitivity prediction (General)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "3UuJS9nbUaEo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cbabbd1a-ec1d-40b9-b181-00d002982354"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8462647050183483"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ],
      "source": [
        "\n",
        "ri1_all= [r.item() for ri in ri1_all for r in ri]\n",
        "ri1_true_all = [r for ri in ri1_true_all for r in ri]\n",
        "\n",
        "ri2_all= [r.item() for ri in ri2_all for r in ri]\n",
        "ri2_true_all = [r for ri in ri2_true_all for r in ri]\n",
        "\n",
        "ri_all=ri1_all+ri2_all\n",
        "ri_true_all=ri1_true_all+ri2_true_all\n",
        "#NDCG\n",
        "dcg_score(ri_all,ri_true_all, k=20)/dcg_score(ri_true_all,ri_true_all, k=20)\n",
        "#AUC\n",
        "metrics.roc_auc_score(ri_true_all,  1/(1 + np.exp(-np.array(ri_all))))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sUPPt-OXUgaq"
      },
      "source": [
        "#Transfer the general model to specific model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGL7sNYvUj16"
      },
      "source": [
        "If you want to boost a bit more with general model's test set\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T9rkoScmUl2_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "916e87a3-9e15-4384-97d2-c997fad6deb9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch  10 |   100/   17 batches | ms/batch 87.88 | loss  0.23159\n",
            "| epoch  10 |   200/   17 batches | ms/batch 87.01 | loss  0.20515\n",
            "| epoch  10 |   300/   17 batches | ms/batch 85.83 | loss  0.21154\n",
            "| epoch  10 |   400/   17 batches | ms/batch 89.06 | loss  0.20682\n",
            "| epoch  10 |   500/   17 batches | ms/batch 88.33 | loss  0.19914\n",
            "| epoch  10 |   100/   17 batches | ms/batch 89.85 | loss  0.16814\n",
            "| epoch  10 |   200/   17 batches | ms/batch 85.97 | loss  0.16068\n",
            "| epoch  10 |   300/   17 batches | ms/batch 88.43 | loss  0.16447\n",
            "| epoch  10 |   400/   17 batches | ms/batch 88.77 | loss  0.15749\n",
            "| epoch  10 |   500/   17 batches | ms/batch 84.70 | loss  0.15284\n"
          ]
        }
      ],
      "source": [
        "#Use major's test set\n",
        "training(False, test_loader)\n",
        "training(True, test_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hzyi-jHvUqHr"
      },
      "source": [
        "##Freeze layers\n",
        "\n",
        "Examine the layer's ID that we'd like to fix or free"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C7oVO97rUnkx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b2755b9-4975-42f4-d12a-f2ff007dc6a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 torch.Size([52, 52]) True\n",
            "1 torch.Size([156, 52]) True\n",
            "2 torch.Size([156]) True\n",
            "3 torch.Size([52, 52]) True\n",
            "4 torch.Size([52]) True\n",
            "5 torch.Size([2048, 52]) True\n",
            "6 torch.Size([2048]) True\n",
            "7 torch.Size([52, 2048]) True\n",
            "8 torch.Size([52]) True\n",
            "9 torch.Size([52]) True\n",
            "10 torch.Size([52]) True\n",
            "11 torch.Size([52]) True\n",
            "12 torch.Size([52]) True\n",
            "13 torch.Size([156, 52]) True\n",
            "14 torch.Size([156]) True\n",
            "15 torch.Size([52, 52]) True\n",
            "16 torch.Size([52]) True\n",
            "17 torch.Size([2048, 52]) True\n",
            "18 torch.Size([2048]) True\n",
            "19 torch.Size([52, 2048]) True\n",
            "20 torch.Size([52]) True\n",
            "21 torch.Size([52]) True\n",
            "22 torch.Size([52]) True\n",
            "23 torch.Size([52]) True\n",
            "24 torch.Size([52]) True\n",
            "25 torch.Size([32, 167]) True\n",
            "26 torch.Size([32]) True\n",
            "27 torch.Size([32, 64]) True\n",
            "28 torch.Size([32]) True\n",
            "29 torch.Size([1, 1, 1, 52]) True\n",
            "30 torch.Size([1]) True\n",
            "31 torch.Size([352]) True\n",
            "32 torch.Size([352]) True\n",
            "33 torch.Size([176, 352]) True\n",
            "34 torch.Size([176]) True\n",
            "35 torch.Size([64, 176]) True\n",
            "36 torch.Size([64]) True\n",
            "37 torch.Size([28, 28]) True\n",
            "38 torch.Size([64, 128]) True\n",
            "39 torch.Size([64]) True\n",
            "40 torch.Size([92]) True\n",
            "41 torch.Size([92]) True\n",
            "42 torch.Size([46, 92]) True\n",
            "43 torch.Size([46]) True\n",
            "44 torch.Size([64, 46]) True\n",
            "45 torch.Size([64]) True\n",
            "46 torch.Size([192]) True\n",
            "47 torch.Size([192]) True\n",
            "48 torch.Size([96, 192]) True\n",
            "49 torch.Size([96]) True\n",
            "50 torch.Size([1, 96]) True\n",
            "51 torch.Size([1]) True\n",
            "52 torch.Size([128]) True\n",
            "53 torch.Size([128]) True\n",
            "54 torch.Size([64, 128]) True\n",
            "55 torch.Size([64]) True\n",
            "56 torch.Size([1, 64]) True\n",
            "57 torch.Size([1]) True\n"
          ]
        }
      ],
      "source": [
        "for i, param in enumerate(model.parameters()):\n",
        "    print(i, param.size(), param.requires_grad)\n",
        "release_after = 46\n",
        "for i, param in enumerate(model.parameters()):\n",
        "    if i>=release_after:\n",
        "        param.requires_grad=True\n",
        "    else:\n",
        "        param.requires_grad=False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9QSZiYiUyyZ"
      },
      "source": [
        "Prostate or bone\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "v0_DRIAjNab_"
      },
      "outputs": [],
      "source": [
        "_train_loader_minor=_train_loader_prostate\n",
        "_test_loader_minor=_test_loader_prostate\n",
        "_test_minor=_test_prostate\n",
        "#_train_loader_minor=_train_loader_bone\n",
        "#_test_loader_minor=_test_loader_bone\n",
        "#_test_minor=_test_bone"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zmBO_7FQUxV9",
        "outputId": "b3ae454c-736c-462b-d0fd-431edaa1361e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "syn mse 0.18647881392594223\n",
            "sen_mse 0.05407539304796156\n",
            "-----------------------------------------------------------------------------------------\n",
            "syn mse 0.13272172278100317\n",
            "sen_mse 0.05336044122884562\n",
            "-----------------------------------------------------------------------------------------\n",
            "syn mse 0.12024202451601133\n",
            "sen_mse 0.05167775625710959\n",
            "-----------------------------------------------------------------------------------------\n",
            "syn mse 0.11358097621372767\n",
            "sen_mse 0.05193812506539481\n",
            "-----------------------------------------------------------------------------------------\n",
            "syn mse 0.10983221871512276\n",
            "sen_mse 0.051680072323306576\n",
            "-----------------------------------------------------------------------------------------\n",
            "syn mse 0.10576765877859932\n",
            "sen_mse 0.05199513592562833\n",
            "-----------------------------------------------------------------------------------------\n",
            "syn mse 0.10389151939978966\n",
            "sen_mse 0.05193541600153996\n",
            "-----------------------------------------------------------------------------------------\n",
            "syn mse 0.10244132660247468\n",
            "sen_mse 0.05179897245469984\n",
            "-----------------------------------------------------------------------------------------\n",
            "syn mse 0.10027597238729288\n",
            "sen_mse 0.05152076679271656\n",
            "-----------------------------------------------------------------------------------------\n",
            "syn mse 0.0995721188220349\n",
            "sen_mse 0.051157469277853494\n",
            "-----------------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "#Use minor's train set\n",
        "try:\n",
        "    for epoch in range(1, epochs+1):\n",
        "        epoch_start_time = time.time()\n",
        "        training(False, _train_loader_minor)\n",
        "        training(True, _train_loader_minor)\n",
        "        evaluate(_test_loader_minor)\n",
        "        print('-'*89)\n",
        "except KeyboardInterrupt:\n",
        "    print('-'*89)\n",
        "    print('Existing from training early')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate synergy prediction (No transfer Prostate)\n"
      ],
      "metadata": {
        "id": "pHWch3IdQSB1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load(data_path+'15-Pooled-no-transfer-prostate.p'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "341vpSP6Ontb",
        "outputId": "ff9636ef-4fe8-400f-c962-ab43840b5eaf"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "syn_all, syn_true_all, ri1_all, ri1_true_all, ri2_all, ri2_true_all= evaluate_accuracy(_test_loader_minor)\n",
        "\n",
        "syn_all= [s.item() for syn in syn_all for s in syn]\n",
        "syn_true_all = [s for syn in syn_true_all for s in syn]\n",
        "#NDCG\n",
        "print(dcg_score(syn_all,syn_true_all, k=20)/dcg_score(syn_true_all,syn_true_all, k=20))\n",
        "#AUROC\n",
        "print(metrics.roc_auc_score(syn_true_all,  1/(1 + np.exp(-np.array(syn_all)))))\n",
        "#AUPRC\n",
        "print(metrics.average_precision_score(syn_true_all,  1/(1 + np.exp(-np.array(syn_all)))))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jkN27mTqQRI9",
        "outputId": "c33118fb-45a4-4242-87a3-817f61954df1"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.0\n",
            "0.986630348869761\n",
            "0.9114650739035156\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate sensitivity prediction (No transfer Prostate)\n"
      ],
      "metadata": {
        "id": "TXzJ8K_tQiih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ri1_all= [r.item() for ri in ri1_all for r in ri]\n",
        "ri1_true_all = [r for ri in ri1_true_all for r in ri]\n",
        "\n",
        "ri2_all= [r.item() for ri in ri2_all for r in ri]\n",
        "ri2_true_all = [r for ri in ri2_true_all for r in ri]\n",
        "\n",
        "ri_all=ri1_all+ri2_all\n",
        "ri_true_all=ri1_true_all+ri2_true_all\n",
        "#NDCG\n",
        "dcg_score(ri_all,ri_true_all, k=20)/dcg_score(ri_true_all,ri_true_all, k=20)\n",
        "#AUC\n",
        "metrics.roc_auc_score(ri_true_all,  1/(1 + np.exp(-np.array(ri_all))))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r1tzo4QIQc4Y",
        "outputId": "20646e17-2cd4-499f-a467-90cfcee12fb7"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7897974169720681"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okJOyNXQU5q4"
      },
      "source": [
        "## Evaluate synergy prediction (Transfer Retrain All Parameters Prostate)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load(data_path+'15-Pooled-transfer-retrain-prostate.p'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xTKk83jRS94_",
        "outputId": "a7707e0a-803d-4a84-cdd0-9ceefdcf7da5"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jAXDRgpzU3RS",
        "outputId": "8ffed9b7-c194-46f1-bb6a-ddd6f5904a75"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.0\n",
            "0.9969655452679639\n",
            "0.9813636350945647\n"
          ]
        }
      ],
      "source": [
        "syn_all, syn_true_all, ri1_all, ri1_true_all, ri2_all, ri2_true_all= evaluate_accuracy(_test_loader_minor)\n",
        "\n",
        "syn_all= [s.item() for syn in syn_all for s in syn]\n",
        "syn_true_all = [s for syn in syn_true_all for s in syn]\n",
        "#NDCG\n",
        "print(dcg_score(syn_all,syn_true_all, k=20)/dcg_score(syn_true_all,syn_true_all, k=20))\n",
        "#AUROC\n",
        "print(metrics.roc_auc_score(syn_true_all,  1/(1 + np.exp(-np.array(syn_all)))))\n",
        "#AUPRC\n",
        "print(metrics.average_precision_score(syn_true_all,  1/(1 + np.exp(-np.array(syn_all)))))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RPX_ogrcVBNE"
      },
      "source": [
        "## Evaluate sensitivity prediction (Transfer Retrain All Parameters Prostate)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tvn89eKpU_Fv",
        "outputId": "8dd0eeb8-fbc6-4915-905e-c666aa8830eb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8036510078963031"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ],
      "source": [
        "ri1_all= [r.item() for ri in ri1_all for r in ri]\n",
        "ri1_true_all = [r for ri in ri1_true_all for r in ri]\n",
        "\n",
        "ri2_all= [r.item() for ri in ri2_all for r in ri]\n",
        "ri2_true_all = [r for ri in ri2_true_all for r in ri]\n",
        "\n",
        "ri_all=ri1_all+ri2_all\n",
        "ri_true_all=ri1_true_all+ri2_true_all\n",
        "#NDCG\n",
        "dcg_score(ri_all,ri_true_all, k=20)/dcg_score(ri_true_all,ri_true_all, k=20)\n",
        "#AUC\n",
        "metrics.roc_auc_score(ri_true_all,  1/(1 + np.exp(-np.array(ri_all))))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0oYI5wdhVG6M"
      },
      "source": [
        "#Select the top ranked drug combinations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XZY6gV7TVFCB",
        "outputId": "82c5eefd-18dc-4b44-e376-5b9d43c8f7e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5-Fluoro-2'-deoxyuridine , Trisenox , DU-145 , prostate , prostate , 0 , 0 , 1.0\n",
            "cyclophosphamide , Trisenox , DU-145 , prostate , prostate , 0 , 0 , 1.0\n",
            "Nilotinib , Trisenox , DU-145 , prostate , prostate , 0 , 0 , 1.0\n",
            "Pralatrexate , Trisenox , DU-145 , prostate , prostate , 0 , 0 , 1.0\n",
            "Procarbazine hydrochloride , Trisenox , DU-145 , prostate , prostate , 0 , 0 , 1.0\n",
            "6-Mercaptopurine , Trisenox , DU-145 , prostate , prostate , 0 , 0 , 1.0\n",
            "5-Fluoro-2'-deoxyuridine , Trisenox , PC-3 , prostate , prostate , 0 , 0 , 1.0\n",
            "MK-2206 , topotecan , LNCAP , prostate , prostate , 0 , 1.0 , 1.0\n",
            "Bortezomib , Trisenox , DU-145 , prostate , prostate , 0 , 0 , 1.0\n",
            "122111-05-1 , Trisenox , DU-145 , prostate , prostate , 0 , 0 , 1.0\n",
            "Eloxatin (TN) (Sanofi Synthelab) , Trisenox , DU-145 , prostate , prostate , 0 , 0 , 1.0\n",
            "Antibiotic AY 22989 , Trisenox , DU-145 , prostate , prostate , 0 , 0 , 1.0\n",
            "Nilotinib , Trisenox , PC-3 , prostate , prostate , 0 , 0 , 1.0\n",
            "6-Mercaptopurine , Ixabepilone , PC-3 , prostate , prostate , 0 , 0 , 1.0\n",
            "cyclophosphamide , Ixabepilone , PC-3 , prostate , prostate , 0 , 0 , 1.0\n",
            "Pralatrexate , Trisenox , PC-3 , prostate , prostate , 0 , 0 , 1.0\n",
            "cyclophosphamide , Trisenox , PC-3 , prostate , prostate , 0 , 0 , 1.0\n",
            "6-Mercaptopurine , Trisenox , PC-3 , prostate , prostate , 0 , 0 , 1.0\n",
            "Procarbazine hydrochloride , Trisenox , PC-3 , prostate , prostate , 0 , 0 , 1.0\n",
            "cyclophosphamide , Ixabepilone , DU-145 , prostate , prostate , 0 , 0 , 1.0\n"
          ]
        }
      ],
      "source": [
        "syn_all_prob=1/(1 + np.exp(-np.array(syn_all)))\n",
        "order = np.argsort(syn_all_prob)[::-1]\n",
        "syn_true_all_order = np.take(syn_true_all, order[:20])\n",
        "for k in range(20):\n",
        "    comb=_test_minor[order[k]]\n",
        "    print(codes['drugs'].idx2item[comb['d1']], ',',\n",
        "          codes['drugs'].idx2item[comb['d2']], ',',\n",
        "          codes['cell'].idx2item[comb['cell']],  ',',\n",
        "          codes['tissue'].idx2item[comb['c_ts']],  ',',\n",
        "          codes['disease'].idx2item[comb['c_ds']], ',',\n",
        "          comb['ri_d1'], ',',\n",
        "          comb['ri_d2'], ',',\n",
        "          syn_true_all_order[k])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NkHC7u_W0V8f"
      },
      "source": [
        "# Results\n",
        "\n",
        "This section will go over the results of the project in comparison to the results of the paper.\n",
        "\n",
        "| Model    | AUROC    | AUPRC    |\n",
        "|----------|----------|----------|\n",
        "|   Original General  |   .9577  |   .8335  |\n",
        "|   Reproduced General  |   .9258  |   .7270  |\n",
        "|   Original Prostate No Transfer (ALMANC,ONEIL)  |   .9775  |   .8575  |\n",
        "|   Reproduced Prostate No Transfer (ALMANC,ONEIL)  |   .9866  |   .9115  |\n",
        "|   Original Prostate Transfer Retrain (ALMANC,ONEIL)  |   .9928  |   .9628  |\n",
        "|   Reproduced Prostate Transfer Retrain (ALMANC,ONEIL)  |   .9970 |   .9814  |\n",
        "\n",
        "As shown by the table we have achieved similar results with the ALMANAC and ONEIL datasets. The original paper hypothesis was given data rich tissue we can use transfer learning on the model in order to imporve the results of drug synergy prediction. The orginal paper showed that with an increase in the AUROC and AUPRC scores. We have also shown an increase in both of those metrics after the transfer learning. Therefore this shows as evidence that transfer learning can be applied to drug synergy prediction models.\n",
        "\n",
        "The goal of the paper was to train a general model and use whatever data was availble for a data poor tissue and use transfer learning to make a drug synergy prediction model for that data poor tissue. We have shown strong evidence that supports the hypothesis of the orginal paper. With these evaluation metrics we have proof that our model can perform effective drug synergy prediction on prostate tissue which is data poor.\n",
        "\n",
        "## Ablation Study\n",
        "\n",
        "We plan to remove the dropout layer for the model. We believe that over-fitting is not necessarily possible with this scenario as this model can be trained and targeted to different forms of human body tissue. Therefore we do not need to worry about this being a general model in totality, we can tailor it to one tissue at a time.\n",
        "\n",
        "## Ablation Results\n",
        "\n",
        "When performing this ablation without a dropout in the encoder models we found that the model became overfitted to the pooled databases. To retiterate, because the data pool was varied in the tissues included we believe that the encoding for the drug and tissues would represent the data well. However we observed that the model did not perform as well at all.\n",
        "\n",
        "Our results for the general model appear to be roughly the same, our AUROC was .9356 and our AUPRC was .8122. This appeared to be fine but after transfer learning we saw a decrease in results. We tested the transfer learning in the same manner the general model was transfered and retrained and retested again prostate tissue. We found that our AUROC was .7148 and our AUPRC was .6520. This is roughly 30% decrease in score from the original papers results.\n",
        "\n",
        "The takeaway is that the dropout is a necessary hypermparameter to the performance of the model. We suspect that because the data rich tissue is multiple times more abundant than the data poor datasets overfitting is all but guaranteed to happen. Transfer learning relies on the model being general enough such that new hyperparameters can be trained. However, we see that the general model requires the dropout to not be overfitted."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Discussion\n",
        "The original paper was able to be somewhat reproduced. The paper used many databases and applied different learning methods to obtain drug synergy score.\n",
        "We were able to use and demonstrate a few databases namely those using prostate tissue. We were able to achieve results close enough to the original paper results. There was some discrepancy which will be discussed further.\n",
        "\n",
        "Predominantly the paper was reproducible and with more time, we may have achieved the exact same results of the original author. Using Google Colab we had all the hardware we need in order to run the code and evaluate the model. All the data was accessicible and even the author was able to be reached via communication. The source code was up to date and fit into modern python and programming standards, and the github was public. For the purpose of reproduction this paper had all the fundamentals to be reproducible.\n",
        "\n",
        "However there are some things that could not be reproduced given the time spent on the project. We could not use all the datasets used to train different models on different tissue. The reason for this is because the datasets provided and the code expectation of the input were not synchronized. There were elements in the input that the code looked for that were not in the datasets themselves. Furthermore, there was an expectation that the reader would know where to load in the data, and where to make changes, in order to run different datasets. This was not clear however, and would take a lot of experience in the deep learning for healthcare field for it to be recognizable.\n",
        "\n",
        "One of the easiest parts was acquiring the data and building the preprocessing file. The preprocessing file was easy because the data that we got from the author slipped fairly easily into the provided code. Using the graphs we were able to understand and report on the data. Another part that was fairly easy was running the given code. There was minimal editing required on our part once the data had undergone the necessary preprocessing.\n",
        "\n",
        "One of the difficult parts was using the data in computation. The lack of clarity on how to use the different datasets was a large challenge. There was also some additional preprocessing on our end that needed to be done. For example disease name and tissue name columns were missing in the cell file. We had to incorporate these through some adhoc method.\n",
        "\n",
        "To improve the reproducibility we would ask the authors to add more information on how to use the data in various ways, and tell the readers what parts of the dataset they are targetting. Another possible improvement would be storing versioned files. The current drugcomb file we used is several versions ahead of the one the paper used. Having the original version would have helped compare results more accurately."
      ],
      "metadata": {
        "id": "uYCODwRByW8B"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "594F3Dhe1PP3"
      },
      "source": [
        "#References\n",
        "\n",
        "1.   Jaeger, S., Duran-Frigola, M. & Aloy, P. Drug sensitivity in cancer cell lines is not tissue-specific.\n",
        "Mol Cancer 14, 40 (2015).\n",
        "https://doi-org.proxy2.library.illinois.edu/10.1186/s12943-015-0312-6\n",
        "2.   Kim, Y., Zheng, S., Tang, J., Jim Zheng, W., Li, Z., & Jiang, X. (2021). Anticancer drug synergy\n",
        "prediction in understudied tissues using transfer learning. Journal of the American\n",
        "Medical Informatics Association : JAMIA, 28(1), 4251.\n",
        "https://doi.org/10.1093/jamia/ocaa212\n",
        "3. Yao F, Madani Tonekaboni SA, Safikhani Z, Smirnov P, El-Hachem N, Freeman M, Manem VSK,\n",
        "Haibe-Kains B. Tissue specificity of in vitro drug sensitivity. J Am Med Inform Assoc.\n",
        "2018 Feb 1;25(2):158-166. doi: 10.1093/jamia/ocx062. PMID: 29016819; PMCID:\n",
        "PMC6381764.\n",
        "4. Zheng, S., Aldahdooh, J., Shadbahr, T., Wang, Y., Aldahdooh, D., Bao, J., Wang, W., & Tang, J.\n",
        "(2021). Drugcomb update: A more comprehensive drug sensitivity data repository and\n",
        "Analysis Portal. Nucleic Acids Research, 49(W1). https://doi.org/10.1093/nar/gkab438\n",
        "5. Kim, Y. (2024). Genetic Features, https://drive.google.com/drive/folders/1Dg8X_R6-qyfjEtgAYGw7RAo94-6JhfNp?usp=sharing"
      ]
    }
  ]
}